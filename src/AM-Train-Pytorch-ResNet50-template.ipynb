{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch code percolation model with ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameter choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L20-100-s100 \n",
      " /storage/disqs/ML-Data/Anderson/Images/L20-100-s100\n",
      "PyTorch-resnet50-111111-e10 \n",
      " Model_PyTorch-resnet50-111111-e10_L20-100-s100.pth \n",
      " History_PyTorch-resnet50-111111-e10_L20-100-s100.pkl\n",
      "./L20-100-s100/ ./L20-100-s100/Model_PyTorch-resnet50-111111-e10_L20-100-s100.pth ./L20-100-s100/History_PyTorch-resnet50-111111-e10_L20-100-s100.pkl\n"
     ]
    }
   ],
   "source": [
    "myseed=111111\n",
    "width= 20\n",
    "nimages= 100\n",
    "img_sizeX= 100\n",
    "img_sizeY= img_sizeX\n",
    "\n",
    "validation_split= 0.1\n",
    "batch_size= 1024\n",
    "num_epochs= 10\n",
    "# mylr= 0.01\n",
    "# mywd= 1e-6\n",
    "\n",
    "dataname='L'+str(width)+'-'+str(nimages)+'-s'+str(img_sizeX)\n",
    "\n",
    "mysavepath = '/MachineLearning-Anderson/src/plots/performance_metrics/System_density_Metric/'\n",
    "devhomepath = '/home/epp/phupqr/PX319MLPhases/'\n",
    "datasrc  = '/storage/disqs/'+'ML-Data/Anderson/Images/'\n",
    "datapath = datasrc+dataname # SC-RTP\n",
    "\n",
    "#mysavepath = '/MachineLearning-Anderson/src/plots/performance_metrics/System_density_Metric/'  ####\n",
    "#devhomepath = '/home/epp/phupqr/PX319MLPhases/'                                              ####\n",
    "#datasrc  = '/Users/danlo/Documents/PX319MLPhases_Data/'\n",
    "#datapath = datasrc+dataname # Local Machine (Laptop)\n",
    "    \n",
    "print(dataname,\"\\n\",datapath)\n",
    "\n",
    "method='PyTorch-resnet50-'+str(myseed)+'-e'+str(num_epochs) #+'-bs'+str(batch_size)\n",
    "modelname = 'Model_'+method+'_'+dataname+'.pth'\n",
    "historyname = 'History_'+method+'_'+dataname+'.pkl'\n",
    "print(method,\"\\n\",modelname,\"\\n\",historyname)\n",
    "\n",
    "savepath = './'+dataname+'/'\n",
    "import os\n",
    "try:\n",
    "    os.mkdir(savepath)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "modelpath = savepath+modelname\n",
    "historypath = savepath+historyname\n",
    "print(savepath,modelpath,historypath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed=42\n",
    "import torch\n",
    "torch.manual_seed(myseed)\n",
    "import numpy as np\n",
    "np.random.seed(myseed+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.2.0\n",
      "sklearn version: 0.21.3\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "# import os\n",
    "print('torch version:',torch.__version__)\n",
    "import sklearn\n",
    "print('sklearn version:', sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device:  cpu torch.float32 torch.strided\n",
      "chosen device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "t=torch.Tensor()\n",
    "print('current device: ', t.device, t.dtype, t.layout)\n",
    "\n",
    "# switch to GPU if available\n",
    "device=t.device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "print('chosen device: ',device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## starting the main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L50-1000-s200', 'L50-5000-s200', 'L50-1000-s300', 'L30-500-s500', 'L50-1000-s100', 'L50-5000-s100', 'L20-5000-s100', 'L30-5000-s100', 'L40-5000-s100', 'L30-500-s100', 'L50-2000-s100', 'L30-500-s200', 'L20-100-s100']\n",
      "[10000.0, 10000.0, 15000.0, 15000.0, 5000.0, 5000.0, 2000.0, 3000.0, 4000.0, 3000.0, 5000.0, 6000.0, 2000.0]\n"
     ]
    }
   ],
   "source": [
    "#Finding the image categories to train on\n",
    "\n",
    "image_categories = []\n",
    "pixel_density = []\n",
    "for entry in os.scandir(datasrc):\n",
    "    image_name = entry.path.rsplit('/', 1)[1]\n",
    "\n",
    "    if image_name[0] == 'L':\n",
    "        pixel_num = float(image_name.rsplit('s', 1)[1])\n",
    "        size_num = 0\n",
    "        size_bool = re.search('L(.+?)-', image_name)\n",
    "        if size_bool:\n",
    "            size_num = int(size_bool.group(1))\n",
    "        pixel_dens = pixel_num*size_num\n",
    "        pixel_density.append(pixel_dens)\n",
    "        image_categories.append(image_name)\n",
    "print(image_categories)\n",
    "print(pixel_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_sizeX= 100\n",
    "# img_sizeY= 100 #img_sizeX\n",
    "# validation_split= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyImageFolder2(torchvision.datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(MyImageFolder2, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        landmarks = landmarks * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'landmarks': landmarks}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'landmarks': torch.from_numpy(landmarks)}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "training_set=0\n",
    "validation_set=0\n",
    "# dataname='L20-100-s100'\n",
    "# path='../../../../../../../media/phrhmb/Datasets_DisQS/'+dataname\n",
    "# path='/storage/disqs/'+'ML-Data/Anderson/Images/'+dataname\n",
    "\n",
    "transform=transforms.Compose([torchvision.transforms.Resize((32,32)),\n",
    "                              torchvision.transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/epp/phupqr/PX319MLPhases/MachineLearning-Anderson/src'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#path = datapath \n",
    "whole_dataset=MyImageFolder2(root=datapath, transform=transform)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_size = len(whole_dataset)\n",
    "print(data_size)\n",
    "# validation_split=0.1\n",
    "split=int(np.floor(validation_split*data_size))\n",
    "training=int(data_size-split)\n",
    "# split the data into training and validation\n",
    "training_set, validation_set= torch.utils.data.random_split(whole_dataset,(training,split))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# batch_size=1024\n",
    "train = torch.utils.data.DataLoader(\n",
    "        dataset=training_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "# enum_train=enumerate(train)\n",
    "# len_train=len(train)\n",
    "\n",
    "val = torch.utils.data.DataLoader(\n",
    "        dataset=validation_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False)\n",
    "# enum_val=enumerate(val)\n",
    "# len_val=len(val)\n",
    "# print(len_train,len_val)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class_names =whole_dataset.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inputs,labels,path= next(iter(val))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "img_sizeX,img_sizeY= inputs.shape[-1],inputs.shape[-2]\n",
    "img_sizeY"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_of_train_samples = len(training_set) # total training samples\n",
    "num_of_test_samples = len(validation_set) #total validation samples\n",
    "steps_per_epoch = np.ceil(num_of_train_samples // batch_size)\n",
    "len_train=len(train)\n",
    "len_val=len(val)\n",
    "number_classes = len(class_names)\n",
    "print('number of samples in the training set:', num_of_train_samples)\n",
    "print('number of samples in the validation set:', num_of_test_samples )\n",
    "print('number of samples in a batch',len_train) \n",
    "print('number of samples in a batch',len_val)\n",
    "print('number of classes',number_classes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_new_training_data(datapath):\n",
    "\n",
    "    global training_set\n",
    "    training_set=0\n",
    "    global validation_set\n",
    "    validation_set=0\n",
    "    # dataname='L20-100-s100'\n",
    "    # path='../../../../../../../media/phrhmb/Datasets_DisQS/'+dataname\n",
    "    # path='/storage/disqs/'+'ML-Data/Anderson/Images/'+dataname\n",
    "\n",
    "    global transform\n",
    "    transform=transforms.Compose([torchvision.transforms.Resize((32,32)),\n",
    "                                  torchvision.transforms.ToTensor()])\n",
    "\n",
    "\n",
    "    #path = datapath \n",
    "    global whole_dataset\n",
    "    whole_dataset=MyImageFolder2(root=datapath, transform=transform)\n",
    "\n",
    "\n",
    "    global data_size\n",
    "    data_size = len(whole_dataset)\n",
    "    print(data_size)\n",
    "    # validation_split=0.1\n",
    "    global split\n",
    "    split=int(np.floor(validation_split*data_size))\n",
    "    global training\n",
    "    training=int(data_size-split)\n",
    "    # split the data into training and validation\n",
    "    training_set, validation_set= torch.utils.data.random_split(whole_dataset,(training,split))\n",
    "\n",
    "\n",
    "    # batch_size=1024\n",
    "    global train\n",
    "    train = torch.utils.data.DataLoader(\n",
    "            dataset=training_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True)\n",
    "    # enum_train=enumerate(train)\n",
    "    # len_train=len(train)\n",
    "\n",
    "    global val\n",
    "    val = torch.utils.data.DataLoader(\n",
    "            dataset=validation_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False)\n",
    "    # enum_val=enumerate(val)\n",
    "    # len_val=len(val)\n",
    "    # print(len_train,len_val)\n",
    "\n",
    "\n",
    "    global class_names\n",
    "    class_names =whole_dataset.classes\n",
    "    class_names\n",
    "\n",
    "\n",
    "    global inputs,labels,path\n",
    "    inputs,labels,path= next(iter(val))\n",
    "    labels.shape\n",
    "\n",
    "\n",
    "    global img_sizeX,img_sizeY\n",
    "    img_sizeX,img_sizeY= inputs.shape[-1],inputs.shape[-2]\n",
    "    img_sizeY\n",
    "\n",
    "\n",
    "    global num_of_train_samples\n",
    "    num_of_train_samples = len(training_set) # total training samples\n",
    "    global num_of_test_samples\n",
    "    num_of_test_samples = len(validation_set) #total validation samples\n",
    "    global steps_per_epoch\n",
    "    steps_per_epoch = np.ceil(num_of_train_samples // batch_size)\n",
    "    global len_train\n",
    "    len_train=len(train)\n",
    "    global len_val\n",
    "    len_val=len(val)\n",
    "    global number_classes\n",
    "    number_classes= len(class_names)\n",
    "    print('number of samples in the training set:', num_of_train_samples)\n",
    "    print('number of samples in the validation set:', num_of_test_samples )\n",
    "    print('number of samples in a batch',len_train) \n",
    "    print('number of samples in a batch',len_val)\n",
    "    print('number of classes',number_classes )\n",
    "    \n",
    "    global model\n",
    "    model=models.resnet50(pretrained=True, progress=True)\n",
    "\n",
    "    global num_ftrs\n",
    "    num_ftrs = model.fc.in_features # number of input features of the last layer which is fully connected (fc)\n",
    "\n",
    "    #We modify the last layer in order to have 2 output: percolating or not\n",
    "    model.fc=nn.Linear(num_ftrs, number_classes )\n",
    "     #the model is sent to the GPU\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # defining the optimizer\n",
    "\n",
    "    global optimizer\n",
    "    optimizer=torch.optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "    # defining the loss function\n",
    "    global criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    global exp_lr_scheduler\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    # checking if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    #print(model)\n",
    "    \n",
    "    #the model is sent to the GPU\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building the CNN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model=models.resnet50(pretrained=True, progress=True)\n",
    "\n",
    "num_ftrs = model.fc.in_features # number of input features of the last layer which is fully connected (fc)\n",
    "\n",
    "#We modify the last layer in order to have 2 output: percolating or not\n",
    "model.fc=nn.Linear(num_ftrs, number_classes )\n",
    " #the model is sent to the GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# defining the optimizer\n",
    "\n",
    "optimizer=torch.optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "# defining the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print model's state_dict\n",
    "# print(\"Model's state_dict:\")\n",
    "# for param_tensor in model.state_dict():\n",
    "#     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# # Print optimizer's state_dict\n",
    "# print(\"Optimizer's state_dict:\")\n",
    "# for var_name in optimizer.state_dict():\n",
    "#     print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.parameters():\n",
    "#     print(param.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#the model is sent to the GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,criterion, optimizer, num_epochs, scheduler, batch_size):\n",
    "    global accuracy, _loss, val_accuracy, val_loss, epochs, val_epochs \n",
    "    since=time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    accuracy=[]\n",
    "    _loss=[]\n",
    "    val_accuracy=[]\n",
    "    val_loss=[]\n",
    "    epochs=[]\n",
    "    val_epochs=[]\n",
    "       \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1,num_epochs))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        #two phases: training and validating\n",
    "        for phase in [train,val]:\n",
    "            if phase == train:\n",
    "                print('Training', end=\" \")\n",
    "                model.train() # set the model to training mode\n",
    "                batches= len_train\n",
    "            else:\n",
    "                print('Validation', end=\" \")\n",
    "                model.eval() # set the model to evaluation mode\n",
    "                batches= len_val\n",
    "                \n",
    "            #batches= len(list(enum_data))\n",
    "            print('with', batches, 'batches')\n",
    "            running_loss=0.0\n",
    "            running_corrects=0.0\n",
    "            \n",
    "            # Here's where the training happens\n",
    "            # print('--- iterating through data ...')\n",
    "            \n",
    "            for i, (inputs,labels,paths) in enumerate(phase):\n",
    "                \n",
    "                print(i*100//batches, '%', end=\"\\r\", flush=True)\n",
    "                \n",
    "                inputs=inputs.to(device)\n",
    "                labels=labels.to(device)\n",
    "                #paths=paths.to(device)\n",
    "                                \n",
    "                #put the gradient to zero to avoid accumulation during back propagation\n",
    "                optimizer.zero_grad()\n",
    "                                \n",
    "                #now we need to carry out the forward and backward process in different steps\n",
    "                #First the forward training\n",
    "                #for the training step we need to log the loss\n",
    "                with torch.set_grad_enabled(phase==train):\n",
    "                    outputs=model(inputs)\n",
    "                    _, preds= torch.max(outputs,1)\n",
    "                    loss=criterion(outputs,labels)\n",
    "                \n",
    "                #still for the training phase we need to implement backword process and optimization\n",
    "                \n",
    "                    if phase==train:\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                # We want variables to hold the loss statistics\n",
    "                #loss.item() extract the loss value as float then it is multiply by the batch size\n",
    "                running_loss+=loss.item()*inputs.size(0)\n",
    "                running_corrects+= torch.sum(preds==labels.data)\n",
    "                \n",
    "            if phase == train:\n",
    "                scheduler.step()            \n",
    "            \n",
    "            if phase == train:\n",
    "                epoch_loss= running_loss/len(phase.dataset)\n",
    "                epoch_acc = running_corrects.double()/ len(phase.dataset)\n",
    "                print('{} loss= {:4f}, accuracy= {:4f}'.format(\n",
    "                    'Training result:', epoch_loss, epoch_acc))\n",
    "                accuracy.append(epoch_acc)\n",
    "                _loss.append(epoch_loss)\n",
    "                epochs.append(epoch)\n",
    "                \n",
    "            if phase == val:\n",
    "                epoch_loss= running_loss/len(val.dataset)\n",
    "                epoch_acc = running_corrects.double()/len(val.dataset)\n",
    "                print('{} val_loss= {:4f}, val_accuracy= {:4f}'.format(\n",
    "                    'Validation result:', epoch_loss, epoch_acc))\n",
    "                val_accuracy.append(epoch_acc)\n",
    "                val_loss.append(epoch_loss)\n",
    "                val_epochs.append(epoch)\n",
    "                \n",
    "            # Make a copy of the model if the accuracy on the validation set has improved\n",
    "            if phase == val and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # Now we'll load in the best model weights and return it\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_handeled = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs,labels,paths) in enumerate(val):\n",
    "\n",
    "            inputs=inputs.to(device)\n",
    "            labels=labels.to(device)\n",
    "\n",
    "            outputs = model(inputs) #value of the output neurons\n",
    "            _, preds = torch.max(outputs, 1) #gives the max value and stores in preds the neurons to which it belongs\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_handeled += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_handeled)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}; \\n true label {}; \\n path: {};'.format(class_names[preds[j]] ,\n",
    "                                                                     class_names[labels[j]],paths[j])\n",
    "                            )\n",
    "                imshow(inputs.cpu().data[j])\n",
    "                \n",
    "                if images_handeled == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_misclassified(model, num_images=6): #gives shows only the misclassified images\n",
    "    import re\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs,labels,paths) in enumerate(val):\n",
    "            \n",
    "            inputs=inputs.to(device)\n",
    "            labels=labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs) #value of the output neurons\n",
    "            _, preds = torch.max(outputs, 1) #gives the max value and stores in preds the neurons to which it belongs\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                if labels[j]!=preds[j] and abs(labels[j]-preds[j])>4:\n",
    "                #print(inputs.size()[0])\n",
    "                    images_so_far += 1\n",
    "                    ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                    ax.axis('off')\n",
    "                    ax.set_title('predicted: {}; \\n true label {}; \\n path: {};'.format(class_names[preds[j]] ,\n",
    "                                                                     class_names[labels[j]],paths[j])\n",
    "                            )\n",
    "                    imshow(inputs.cpu().data[j])\n",
    "                \n",
    "                    if images_so_far == num_images:\n",
    "                        model.train(mode=was_training)\n",
    "                        return\n",
    "        \n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000\n",
      "number of samples in the training set: 15300\n",
      "number of samples in the validation set: 1700\n",
      "number of samples in a batch 15\n",
      "number of samples in a batch 2\n",
      "number of classes 17\n",
      "L50-1000-s200 \n",
      " /storage/disqs/ML-Data/Anderson/Images/L50-1000-s200\n",
      "PyTorch-resnet50-111111-e10 \n",
      " Model_PyTorch-resnet50-111111-e10_L50-1000-s200.pth \n",
      " History_PyTorch-resnet50-111111-e10_L50-1000-s200.pkl\n",
      "./L50-1000-s200/ ./L50-1000-s200/Model_PyTorch-resnet50-111111-e10_L50-1000-s200.pth ./L50-1000-s200/History_PyTorch-resnet50-111111-e10_L50-1000-s200.pkl\n",
      "Epoch 1/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.900599, accuracy= 0.110065\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 13.571676, val_accuracy= 0.064706\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.420899, accuracy= 0.162876\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 3.503225, val_accuracy= 0.124706\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.327295, accuracy= 0.176993\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.447985, val_accuracy= 0.152941\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.345030, accuracy= 0.176667\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.731620, val_accuracy= 0.155882\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.306400, accuracy= 0.178497\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.798654, val_accuracy= 0.118235\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.277619, accuracy= 0.182222\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.480817, val_accuracy= 0.143529\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.272032, accuracy= 0.182026\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.472891, val_accuracy= 0.148235\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.175090, accuracy= 0.209804\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.208209, val_accuracy= 0.195882\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.144374, accuracy= 0.217190\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.208463, val_accuracy= 0.190000\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.131877, accuracy= 0.221242\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.217614, val_accuracy= 0.178235\n",
      "\n",
      "Training complete in 8m 23s\n",
      "Best val Acc: 0.195882\n",
      "[tensor(0.0647, device='cuda:0', dtype=torch.float64), tensor(0.1247, device='cuda:0', dtype=torch.float64), tensor(0.1529, device='cuda:0', dtype=torch.float64), tensor(0.1559, device='cuda:0', dtype=torch.float64), tensor(0.1182, device='cuda:0', dtype=torch.float64), tensor(0.1435, device='cuda:0', dtype=torch.float64), tensor(0.1482, device='cuda:0', dtype=torch.float64), tensor(0.1959, device='cuda:0', dtype=torch.float64), tensor(0.1900, device='cuda:0', dtype=torch.float64), tensor(0.1782, device='cuda:0', dtype=torch.float64)]\n",
      "0.17823529411764705\n",
      "85000\n",
      "number of samples in the training set: 76500\n",
      "number of samples in the validation set: 8500\n",
      "number of samples in a batch 75\n",
      "number of samples in a batch 9\n",
      "number of classes 17\n",
      "L50-5000-s200 \n",
      " /storage/disqs/ML-Data/Anderson/Images/L50-5000-s200\n",
      "PyTorch-resnet50-111111-e10 \n",
      " Model_PyTorch-resnet50-111111-e10_L50-5000-s200.pth \n",
      " History_PyTorch-resnet50-111111-e10_L50-5000-s200.pkl\n",
      "./L50-5000-s200/ ./L50-5000-s200/Model_PyTorch-resnet50-111111-e10_L50-5000-s200.pth ./L50-5000-s200/History_PyTorch-resnet50-111111-e10_L50-5000-s200.pkl\n",
      "Epoch 1/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.425973, accuracy= 0.169673\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 24.423311, val_accuracy= 0.142235\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.207060, accuracy= 0.205150\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.277666, val_accuracy= 0.201882\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.147024, accuracy= 0.220784\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.315631, val_accuracy= 0.203412\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.130539, accuracy= 0.221922\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.162934, val_accuracy= 0.212118\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.097594, accuracy= 0.232026\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.440157, val_accuracy= 0.191647\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.072085, accuracy= 0.237373\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 3.152702, val_accuracy= 0.161765\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.055581, accuracy= 0.244418\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.165400, val_accuracy= 0.217765\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 1.963641, accuracy= 0.270052\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.096082, val_accuracy= 0.235529\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 1.938596, accuracy= 0.278444\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.080295, val_accuracy= 0.240588\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 1.921561, accuracy= 0.283007\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.072861, val_accuracy= 0.244235\n",
      "\n",
      "Training complete in 55m 3s\n",
      "Best val Acc: 0.244235\n",
      "[tensor(0.1422, device='cuda:0', dtype=torch.float64), tensor(0.2019, device='cuda:0', dtype=torch.float64), tensor(0.2034, device='cuda:0', dtype=torch.float64), tensor(0.2121, device='cuda:0', dtype=torch.float64), tensor(0.1916, device='cuda:0', dtype=torch.float64), tensor(0.1618, device='cuda:0', dtype=torch.float64), tensor(0.2178, device='cuda:0', dtype=torch.float64), tensor(0.2355, device='cuda:0', dtype=torch.float64), tensor(0.2406, device='cuda:0', dtype=torch.float64), tensor(0.2442, device='cuda:0', dtype=torch.float64)]\n",
      "0.24423529411764708\n",
      "17000\n",
      "number of samples in the training set: 15300\n",
      "number of samples in the validation set: 1700\n",
      "number of samples in a batch 15\n",
      "number of samples in a batch 2\n",
      "number of classes 17\n",
      "L50-1000-s300 \n",
      " /storage/disqs/ML-Data/Anderson/Images/L50-1000-s300\n",
      "PyTorch-resnet50-111111-e10 \n",
      " Model_PyTorch-resnet50-111111-e10_L50-1000-s300.pth \n",
      " History_PyTorch-resnet50-111111-e10_L50-1000-s300.pkl\n",
      "./L50-1000-s300/ ./L50-1000-s300/Model_PyTorch-resnet50-111111-e10_L50-1000-s300.pth ./L50-1000-s300/History_PyTorch-resnet50-111111-e10_L50-1000-s300.pkl\n",
      "Epoch 1/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.908736, accuracy= 0.119281\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 4.570452, val_accuracy= 0.086471\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.402975, accuracy= 0.164379\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.888919, val_accuracy= 0.120000\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.344873, accuracy= 0.176144\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.697869, val_accuracy= 0.147059\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.286911, accuracy= 0.180000\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.775268, val_accuracy= 0.149412\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.296655, accuracy= 0.179020\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.449784, val_accuracy= 0.167059\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.253962, accuracy= 0.190719\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.789223, val_accuracy= 0.134118\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.239825, accuracy= 0.199085\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 59.769798, val_accuracy= 0.060588\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.185766, accuracy= 0.208824\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.165581, val_accuracy= 0.201765\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.143771, accuracy= 0.221373\n",
      "Validation with 2 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result: val_loss= 2.166633, val_accuracy= 0.200000\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.121526, accuracy= 0.225033\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.162621, val_accuracy= 0.194118\n",
      "\n",
      "Training complete in 14m 52s\n",
      "Best val Acc: 0.201765\n",
      "[tensor(0.0865, device='cuda:0', dtype=torch.float64), tensor(0.1200, device='cuda:0', dtype=torch.float64), tensor(0.1471, device='cuda:0', dtype=torch.float64), tensor(0.1494, device='cuda:0', dtype=torch.float64), tensor(0.1671, device='cuda:0', dtype=torch.float64), tensor(0.1341, device='cuda:0', dtype=torch.float64), tensor(0.0606, device='cuda:0', dtype=torch.float64), tensor(0.2018, device='cuda:0', dtype=torch.float64), tensor(0.2000, device='cuda:0', dtype=torch.float64), tensor(0.1941, device='cuda:0', dtype=torch.float64)]\n",
      "0.1941176470588235\n",
      "8501\n",
      "number of samples in the training set: 7651\n",
      "number of samples in the validation set: 850\n",
      "number of samples in a batch 8\n",
      "number of samples in a batch 1\n",
      "number of classes 17\n",
      "L30-500-s500 \n",
      " /storage/disqs/ML-Data/Anderson/Images/L30-500-s500\n",
      "PyTorch-resnet50-111111-e10 \n",
      " Model_PyTorch-resnet50-111111-e10_L30-500-s500.pth \n",
      " History_PyTorch-resnet50-111111-e10_L30-500-s500.pkl\n",
      "./L30-500-s500/ ./L30-500-s500/Model_PyTorch-resnet50-111111-e10_L30-500-s500.pth ./L30-500-s500/History_PyTorch-resnet50-111111-e10_L30-500-s500.pkl\n",
      "Epoch 1/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 3.214396, accuracy= 0.065874\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 30.099203, val_accuracy= 0.063529\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.797248, accuracy= 0.115540\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 3.265291, val_accuracy= 0.063529\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.634020, accuracy= 0.122598\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.709481, val_accuracy= 0.104706\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.567103, accuracy= 0.144033\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.856462, val_accuracy= 0.131765\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.497424, accuracy= 0.151483\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.964877, val_accuracy= 0.145882\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.456575, accuracy= 0.162593\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.536082, val_accuracy= 0.122353\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.454230, accuracy= 0.171350\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.517857, val_accuracy= 0.147059\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.357775, accuracy= 0.198928\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.455086, val_accuracy= 0.148235\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.277169, accuracy= 0.241276\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.469589, val_accuracy= 0.138824\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.230706, accuracy= 0.254607\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.478956, val_accuracy= 0.132941\n",
      "\n",
      "Training complete in 10m 19s\n",
      "Best val Acc: 0.148235\n",
      "[tensor(0.0635, device='cuda:0', dtype=torch.float64), tensor(0.0635, device='cuda:0', dtype=torch.float64), tensor(0.1047, device='cuda:0', dtype=torch.float64), tensor(0.1318, device='cuda:0', dtype=torch.float64), tensor(0.1459, device='cuda:0', dtype=torch.float64), tensor(0.1224, device='cuda:0', dtype=torch.float64), tensor(0.1471, device='cuda:0', dtype=torch.float64), tensor(0.1482, device='cuda:0', dtype=torch.float64), tensor(0.1388, device='cuda:0', dtype=torch.float64), tensor(0.1329, device='cuda:0', dtype=torch.float64)]\n",
      "0.13294117647058823\n",
      "17000\n",
      "number of samples in the training set: 15300\n",
      "number of samples in the validation set: 1700\n",
      "number of samples in a batch 15\n",
      "number of samples in a batch 2\n",
      "number of classes 17\n",
      "L50-1000-s100 \n",
      " /storage/disqs/ML-Data/Anderson/Images/L50-1000-s100\n",
      "PyTorch-resnet50-111111-e10 \n",
      " Model_PyTorch-resnet50-111111-e10_L50-1000-s100.pth \n",
      " History_PyTorch-resnet50-111111-e10_L50-1000-s100.pkl\n",
      "./L50-1000-s100/ ./L50-1000-s100/Model_PyTorch-resnet50-111111-e10_L50-1000-s100.pth ./L50-1000-s100/History_PyTorch-resnet50-111111-e10_L50-1000-s100.pkl\n",
      "Epoch 1/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.775240, accuracy= 0.116863\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 3.959471, val_accuracy= 0.070588\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.415187, accuracy= 0.161373\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 4.758593, val_accuracy= 0.140000\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.312269, accuracy= 0.178497\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.308734, val_accuracy= 0.164118\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.315219, accuracy= 0.180784\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 9.848311, val_accuracy= 0.088235\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.275751, accuracy= 0.180915\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.284122, val_accuracy= 0.153529\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.250472, accuracy= 0.193660\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.289399, val_accuracy= 0.148235\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.234916, accuracy= 0.194052\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.518990, val_accuracy= 0.137647\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.166028, accuracy= 0.215098\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.233336, val_accuracy= 0.175882\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.099400, accuracy= 0.228170\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.245081, val_accuracy= 0.187647\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Training with 15 batches\n",
      "Training result: loss= 2.078991, accuracy= 0.236209\n",
      "Validation with 2 batches\n",
      "Validation result: val_loss= 2.240832, val_accuracy= 0.195882\n",
      "\n",
      "Training complete in 8m 5s\n",
      "Best val Acc: 0.195882\n",
      "[tensor(0.0706, device='cuda:0', dtype=torch.float64), tensor(0.1400, device='cuda:0', dtype=torch.float64), tensor(0.1641, device='cuda:0', dtype=torch.float64), tensor(0.0882, device='cuda:0', dtype=torch.float64), tensor(0.1535, device='cuda:0', dtype=torch.float64), tensor(0.1482, device='cuda:0', dtype=torch.float64), tensor(0.1376, device='cuda:0', dtype=torch.float64), tensor(0.1759, device='cuda:0', dtype=torch.float64), tensor(0.1876, device='cuda:0', dtype=torch.float64), tensor(0.1959, device='cuda:0', dtype=torch.float64)]\n",
      "0.19588235294117645\n",
      "71760\n",
      "number of samples in the training set: 64584\n",
      "number of samples in the validation set: 7176\n",
      "number of samples in a batch 64\n",
      "number of samples in a batch 8\n",
      "number of classes 15\n",
      "L50-5000-s100 \n",
      " /storage/disqs/ML-Data/Anderson/Images/L50-5000-s100\n",
      "PyTorch-resnet50-111111-e10 \n",
      " Model_PyTorch-resnet50-111111-e10_L50-5000-s100.pth \n",
      " History_PyTorch-resnet50-111111-e10_L50-5000-s100.pkl\n",
      "./L50-5000-s100/ ./L50-5000-s100/Model_PyTorch-resnet50-111111-e10_L50-5000-s100.pth ./L50-5000-s100/History_PyTorch-resnet50-111111-e10_L50-5000-s100.pkl\n",
      "Epoch 1/10\n",
      "----------\n",
      "Training with 64 batches\n",
      "Training result: loss= 2.320042, accuracy= 0.188715\n",
      "Validation with 8 batches\n",
      "Validation result: val_loss= 4.004873, val_accuracy= 0.070095\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Training with 64 batches\n",
      "Training result: loss= 2.163434, accuracy= 0.215363\n",
      "Validation with 8 batches\n",
      "Validation result: val_loss= 3.345018, val_accuracy= 0.144091\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Training with 64 batches\n",
      "Training result: loss= 2.151326, accuracy= 0.219172\n",
      "Validation with 8 batches\n",
      "Validation result: val_loss= 3.524781, val_accuracy= 0.103400\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Training with 64 batches\n",
      "Training result: loss= 2.128070, accuracy= 0.225164\n",
      "Validation with 8 batches\n",
      "Validation result: val_loss= 2.918509, val_accuracy= 0.184783\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Training with 64 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result: loss= 2.117805, accuracy= 0.227549\n",
      "Validation with 8 batches\n",
      "Validation result: val_loss= 2.319210, val_accuracy= 0.204013\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Training with 64 batches\n",
      "Training result: loss= 2.101404, accuracy= 0.231512\n",
      "Validation with 8 batches\n",
      "Validation result: val_loss= 279.721067, val_accuracy= 0.065635\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Training with 64 batches\n",
      "Training result: loss= 2.098963, accuracy= 0.229949\n",
      "Validation with 8 batches\n",
      "Validation result: val_loss= 3.741900, val_accuracy= 0.170847\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Training with 64 batches\n",
      "Training result: loss= 2.065574, accuracy= 0.237629\n",
      "Validation with 8 batches\n",
      "Validation result: val_loss= 2.118097, val_accuracy= 0.233696\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Training with 64 batches\n",
      "Training result: loss= 2.019908, accuracy= 0.249288\n",
      "Validation with 8 batches\n",
      "Validation result: val_loss= 2.175121, val_accuracy= 0.229236\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Training with 64 batches\n",
      "Training result: loss= 2.009059, accuracy= 0.250526\n",
      "Validation with 8 batches\n",
      "Validation result: val_loss= 2.316460, val_accuracy= 0.234671\n",
      "\n",
      "Training complete in 42m 53s\n",
      "Best val Acc: 0.234671\n",
      "[tensor(0.0701, device='cuda:0', dtype=torch.float64), tensor(0.1441, device='cuda:0', dtype=torch.float64), tensor(0.1034, device='cuda:0', dtype=torch.float64), tensor(0.1848, device='cuda:0', dtype=torch.float64), tensor(0.2040, device='cuda:0', dtype=torch.float64), tensor(0.0656, device='cuda:0', dtype=torch.float64), tensor(0.1708, device='cuda:0', dtype=torch.float64), tensor(0.2337, device='cuda:0', dtype=torch.float64), tensor(0.2292, device='cuda:0', dtype=torch.float64), tensor(0.2347, device='cuda:0', dtype=torch.float64)]\n",
      "0.2346711259754738\n",
      "85000\n",
      "number of samples in the training set: 76500\n",
      "number of samples in the validation set: 8500\n",
      "number of samples in a batch 75\n",
      "number of samples in a batch 9\n",
      "number of classes 18\n",
      "L20-5000-s100 \n",
      " /storage/disqs/ML-Data/Anderson/Images/L20-5000-s100\n",
      "PyTorch-resnet50-111111-e10 \n",
      " Model_PyTorch-resnet50-111111-e10_L20-5000-s100.pth \n",
      " History_PyTorch-resnet50-111111-e10_L20-5000-s100.pkl\n",
      "./L20-5000-s100/ ./L20-5000-s100/Model_PyTorch-resnet50-111111-e10_L20-5000-s100.pth ./L20-5000-s100/History_PyTorch-resnet50-111111-e10_L20-5000-s100.pkl\n",
      "Epoch 1/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.738256, accuracy= 0.108706\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.896205, val_accuracy= 0.114706\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.600908, accuracy= 0.123451\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.616583, val_accuracy= 0.114824\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.581919, accuracy= 0.128418\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.616312, val_accuracy= 0.119412\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.570984, accuracy= 0.133020\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.821592, val_accuracy= 0.117059\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.558270, accuracy= 0.136170\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.596343, val_accuracy= 0.124706\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.543908, accuracy= 0.143255\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.608554, val_accuracy= 0.114353\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.531245, accuracy= 0.147673\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 3.699511, val_accuracy= 0.122941\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.421916, accuracy= 0.192680\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.639434, val_accuracy= 0.125294\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.342193, accuracy= 0.222222\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.695843, val_accuracy= 0.123765\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.254144, accuracy= 0.253922\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.799274, val_accuracy= 0.116471\n",
      "\n",
      "Training complete in 18m 41s\n",
      "Best val Acc: 0.125294\n",
      "[tensor(0.1147, device='cuda:0', dtype=torch.float64), tensor(0.1148, device='cuda:0', dtype=torch.float64), tensor(0.1194, device='cuda:0', dtype=torch.float64), tensor(0.1171, device='cuda:0', dtype=torch.float64), tensor(0.1247, device='cuda:0', dtype=torch.float64), tensor(0.1144, device='cuda:0', dtype=torch.float64), tensor(0.1229, device='cuda:0', dtype=torch.float64), tensor(0.1253, device='cuda:0', dtype=torch.float64), tensor(0.1238, device='cuda:0', dtype=torch.float64), tensor(0.1165, device='cuda:0', dtype=torch.float64)]\n",
      "0.11647058823529412\n",
      "85000\n",
      "number of samples in the training set: 76500\n",
      "number of samples in the validation set: 8500\n",
      "number of samples in a batch 75\n",
      "number of samples in a batch 9\n",
      "number of classes 18\n",
      "L30-5000-s100 \n",
      " /storage/disqs/ML-Data/Anderson/Images/L30-5000-s100\n",
      "PyTorch-resnet50-111111-e10 \n",
      " Model_PyTorch-resnet50-111111-e10_L30-5000-s100.pth \n",
      " History_PyTorch-resnet50-111111-e10_L30-5000-s100.pkl\n",
      "./L30-5000-s100/ ./L30-5000-s100/Model_PyTorch-resnet50-111111-e10_L30-5000-s100.pth ./L30-5000-s100/History_PyTorch-resnet50-111111-e10_L30-5000-s100.pkl\n",
      "Epoch 1/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.618419, accuracy= 0.129490\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.457351, val_accuracy= 0.144706\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.462206, accuracy= 0.146052\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.517655, val_accuracy= 0.137059\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.439219, accuracy= 0.150745\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.543208, val_accuracy= 0.142471\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.423582, accuracy= 0.151163\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.487684, val_accuracy= 0.148588\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.409940, accuracy= 0.154523\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.471992, val_accuracy= 0.143176\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.397229, accuracy= 0.158824\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.455825, val_accuracy= 0.146000\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.386463, accuracy= 0.157595\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.446281, val_accuracy= 0.147882\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.326867, accuracy= 0.178484\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.418747, val_accuracy= 0.151647\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.304755, accuracy= 0.185464\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.433674, val_accuracy= 0.152000\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.287918, accuracy= 0.189451\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.444540, val_accuracy= 0.149529\n",
      "\n",
      "Training complete in 18m 39s\n",
      "Best val Acc: 0.152000\n",
      "[tensor(0.1447, device='cuda:0', dtype=torch.float64), tensor(0.1371, device='cuda:0', dtype=torch.float64), tensor(0.1425, device='cuda:0', dtype=torch.float64), tensor(0.1486, device='cuda:0', dtype=torch.float64), tensor(0.1432, device='cuda:0', dtype=torch.float64), tensor(0.1460, device='cuda:0', dtype=torch.float64), tensor(0.1479, device='cuda:0', dtype=torch.float64), tensor(0.1516, device='cuda:0', dtype=torch.float64), tensor(0.1520, device='cuda:0', dtype=torch.float64), tensor(0.1495, device='cuda:0', dtype=torch.float64)]\n",
      "0.14952941176470588\n",
      "85000\n",
      "number of samples in the training set: 76500\n",
      "number of samples in the validation set: 8500\n",
      "number of samples in a batch 75\n",
      "number of samples in a batch 9\n",
      "number of classes 18\n",
      "L40-5000-s100 \n",
      " /storage/disqs/ML-Data/Anderson/Images/L40-5000-s100\n",
      "PyTorch-resnet50-111111-e10 \n",
      " Model_PyTorch-resnet50-111111-e10_L40-5000-s100.pth \n",
      " History_PyTorch-resnet50-111111-e10_L40-5000-s100.pkl\n",
      "./L40-5000-s100/ ./L40-5000-s100/Model_PyTorch-resnet50-111111-e10_L40-5000-s100.pth ./L40-5000-s100/History_PyTorch-resnet50-111111-e10_L40-5000-s100.pkl\n",
      "Epoch 1/10\n",
      "----------\n",
      "Training with 75 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result: loss= 2.515190, accuracy= 0.147843\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 5.895854, val_accuracy= 0.116471\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.354018, accuracy= 0.164654\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 350.452607, val_accuracy= 0.104471\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.332497, accuracy= 0.169634\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.497340, val_accuracy= 0.122941\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.313919, accuracy= 0.172431\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.331664, val_accuracy= 0.163294\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.304471, accuracy= 0.174105\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.335725, val_accuracy= 0.179882\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.287268, accuracy= 0.178209\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.365287, val_accuracy= 0.160706\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.286477, accuracy= 0.180876\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.354489, val_accuracy= 0.168000\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.234071, accuracy= 0.192941\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.271728, val_accuracy= 0.179647\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.214970, accuracy= 0.197634\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.272965, val_accuracy= 0.176471\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Training with 75 batches\n",
      "Training result: loss= 2.205335, accuracy= 0.200144\n",
      "Validation with 9 batches\n",
      "Validation result: val_loss= 2.277800, val_accuracy= 0.180118\n",
      "\n",
      "Training complete in 31m 8s\n",
      "Best val Acc: 0.180118\n",
      "[tensor(0.1165, device='cuda:0', dtype=torch.float64), tensor(0.1045, device='cuda:0', dtype=torch.float64), tensor(0.1229, device='cuda:0', dtype=torch.float64), tensor(0.1633, device='cuda:0', dtype=torch.float64), tensor(0.1799, device='cuda:0', dtype=torch.float64), tensor(0.1607, device='cuda:0', dtype=torch.float64), tensor(0.1680, device='cuda:0', dtype=torch.float64), tensor(0.1796, device='cuda:0', dtype=torch.float64), tensor(0.1765, device='cuda:0', dtype=torch.float64), tensor(0.1801, device='cuda:0', dtype=torch.float64)]\n",
      "0.18011764705882352\n",
      "8501\n",
      "number of samples in the training set: 7651\n",
      "number of samples in the validation set: 850\n",
      "number of samples in a batch 8\n",
      "number of samples in a batch 1\n",
      "number of classes 17\n",
      "L30-500-s100 \n",
      " /storage/disqs/ML-Data/Anderson/Images/L30-500-s100\n",
      "PyTorch-resnet50-111111-e10 \n",
      " Model_PyTorch-resnet50-111111-e10_L30-500-s100.pth \n",
      " History_PyTorch-resnet50-111111-e10_L30-500-s100.pkl\n",
      "./L30-500-s100/ ./L30-500-s100/Model_PyTorch-resnet50-111111-e10_L30-500-s100.pth ./L30-500-s100/History_PyTorch-resnet50-111111-e10_L30-500-s100.pkl\n",
      "Epoch 1/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 3.101473, accuracy= 0.082473\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 3.415645, val_accuracy= 0.045882\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.797501, accuracy= 0.113711\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 3.588808, val_accuracy= 0.069412\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.598750, accuracy= 0.136191\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.692168, val_accuracy= 0.120000\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.530992, accuracy= 0.140374\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.518047, val_accuracy= 0.130588\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.478040, accuracy= 0.147562\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.542413, val_accuracy= 0.125882\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.470924, accuracy= 0.147170\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.757068, val_accuracy= 0.132941\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.461993, accuracy= 0.155797\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.572272, val_accuracy= 0.128235\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.390801, accuracy= 0.181414\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.492946, val_accuracy= 0.162353\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.337439, accuracy= 0.196314\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.499411, val_accuracy= 0.160000\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.312197, accuracy= 0.201673\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.504952, val_accuracy= 0.136471\n",
      "\n",
      "Training complete in 4m 24s\n",
      "Best val Acc: 0.162353\n",
      "[tensor(0.0459, device='cuda:0', dtype=torch.float64), tensor(0.0694, device='cuda:0', dtype=torch.float64), tensor(0.1200, device='cuda:0', dtype=torch.float64), tensor(0.1306, device='cuda:0', dtype=torch.float64), tensor(0.1259, device='cuda:0', dtype=torch.float64), tensor(0.1329, device='cuda:0', dtype=torch.float64), tensor(0.1282, device='cuda:0', dtype=torch.float64), tensor(0.1624, device='cuda:0', dtype=torch.float64), tensor(0.1600, device='cuda:0', dtype=torch.float64), tensor(0.1365, device='cuda:0', dtype=torch.float64)]\n",
      "0.1364705882352941\n",
      "34000\n",
      "number of samples in the training set: 30600\n",
      "number of samples in the validation set: 3400\n",
      "number of samples in a batch 30\n",
      "number of samples in a batch 4\n",
      "number of classes 17\n",
      "L50-2000-s100 \n",
      " /storage/disqs/ML-Data/Anderson/Images/L50-2000-s100\n",
      "PyTorch-resnet50-111111-e10 \n",
      " Model_PyTorch-resnet50-111111-e10_L50-2000-s100.pth \n",
      " History_PyTorch-resnet50-111111-e10_L50-2000-s100.pkl\n",
      "./L50-2000-s100/ ./L50-2000-s100/Model_PyTorch-resnet50-111111-e10_L50-2000-s100.pth ./L50-2000-s100/History_PyTorch-resnet50-111111-e10_L50-2000-s100.pkl\n",
      "Epoch 1/10\n",
      "----------\n",
      "Training with 30 batches\n",
      "Training result: loss= 2.701194, accuracy= 0.132386\n",
      "Validation with 4 batches\n",
      "Validation result: val_loss= 22.721928, val_accuracy= 0.075294\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Training with 30 batches\n",
      "Training result: loss= 2.408178, accuracy= 0.163235\n",
      "Validation with 4 batches\n",
      "Validation result: val_loss= 2.556284, val_accuracy= 0.135588\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Training with 30 batches\n",
      "Training result: loss= 2.330241, accuracy= 0.181732\n",
      "Validation with 4 batches\n",
      "Validation result: val_loss= 2.278568, val_accuracy= 0.177941\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Training with 30 batches\n",
      "Training result: loss= 2.265364, accuracy= 0.192647\n",
      "Validation with 4 batches\n",
      "Validation result: val_loss= 2.435243, val_accuracy= 0.186176\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Training with 30 batches\n",
      "Training result: loss= 2.201633, accuracy= 0.208137\n",
      "Validation with 4 batches\n",
      "Validation result: val_loss= 2.188015, val_accuracy= 0.205882\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Training with 30 batches\n",
      "Training result: loss= 2.176713, accuracy= 0.213562\n",
      "Validation with 4 batches\n",
      "Validation result: val_loss= 2.701691, val_accuracy= 0.185000\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Training with 30 batches\n",
      "Training result: loss= 2.160199, accuracy= 0.217026\n",
      "Validation with 4 batches\n",
      "Validation result: val_loss= 2.165851, val_accuracy= 0.210294\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Training with 30 batches\n",
      "Training result: loss= 2.059499, accuracy= 0.246176\n",
      "Validation with 4 batches\n",
      "Validation result: val_loss= 2.105160, val_accuracy= 0.219118\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Training with 30 batches\n",
      "Training result: loss= 2.037476, accuracy= 0.253889\n",
      "Validation with 4 batches\n",
      "Validation result: val_loss= 2.134900, val_accuracy= 0.220588\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Training with 30 batches\n",
      "Training result: loss= 2.026597, accuracy= 0.254412\n",
      "Validation with 4 batches\n",
      "Validation result: val_loss= 2.097979, val_accuracy= 0.226176\n",
      "\n",
      "Training complete in 11m 48s\n",
      "Best val Acc: 0.226176\n",
      "[tensor(0.0753, device='cuda:0', dtype=torch.float64), tensor(0.1356, device='cuda:0', dtype=torch.float64), tensor(0.1779, device='cuda:0', dtype=torch.float64), tensor(0.1862, device='cuda:0', dtype=torch.float64), tensor(0.2059, device='cuda:0', dtype=torch.float64), tensor(0.1850, device='cuda:0', dtype=torch.float64), tensor(0.2103, device='cuda:0', dtype=torch.float64), tensor(0.2191, device='cuda:0', dtype=torch.float64), tensor(0.2206, device='cuda:0', dtype=torch.float64), tensor(0.2262, device='cuda:0', dtype=torch.float64)]\n",
      "0.22617647058823528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8501\n",
      "number of samples in the training set: 7651\n",
      "number of samples in the validation set: 850\n",
      "number of samples in a batch 8\n",
      "number of samples in a batch 1\n",
      "number of classes 17\n",
      "L30-500-s200 \n",
      " /storage/disqs/ML-Data/Anderson/Images/L30-500-s200\n",
      "PyTorch-resnet50-111111-e10 \n",
      " Model_PyTorch-resnet50-111111-e10_L30-500-s200.pth \n",
      " History_PyTorch-resnet50-111111-e10_L30-500-s200.pkl\n",
      "./L30-500-s200/ ./L30-500-s200/Model_PyTorch-resnet50-111111-e10_L30-500-s200.pth ./L30-500-s200/History_PyTorch-resnet50-111111-e10_L30-500-s200.pkl\n",
      "Epoch 1/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 3.129753, accuracy= 0.065482\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 15.528581, val_accuracy= 0.056471\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.773732, accuracy= 0.111750\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 4.125646, val_accuracy= 0.090588\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.648495, accuracy= 0.132270\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 57.747414, val_accuracy= 0.104706\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.660079, accuracy= 0.130833\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 81.391533, val_accuracy= 0.061176\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.547636, accuracy= 0.138544\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 3.836691, val_accuracy= 0.116471\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.524579, accuracy= 0.149654\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.615279, val_accuracy= 0.122353\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.503886, accuracy= 0.157365\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.607371, val_accuracy= 0.120000\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.480516, accuracy= 0.165338\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.491646, val_accuracy= 0.134118\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.391181, accuracy= 0.183898\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.484117, val_accuracy= 0.132941\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Training with 8 batches\n",
      "Training result: loss= 2.350005, accuracy= 0.187165\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.477127, val_accuracy= 0.140000\n",
      "\n",
      "Training complete in 3m 58s\n",
      "Best val Acc: 0.140000\n",
      "[tensor(0.0565, device='cuda:0', dtype=torch.float64), tensor(0.0906, device='cuda:0', dtype=torch.float64), tensor(0.1047, device='cuda:0', dtype=torch.float64), tensor(0.0612, device='cuda:0', dtype=torch.float64), tensor(0.1165, device='cuda:0', dtype=torch.float64), tensor(0.1224, device='cuda:0', dtype=torch.float64), tensor(0.1200, device='cuda:0', dtype=torch.float64), tensor(0.1341, device='cuda:0', dtype=torch.float64), tensor(0.1329, device='cuda:0', dtype=torch.float64), tensor(0.1400, device='cuda:0', dtype=torch.float64)]\n",
      "0.13999999999999999\n",
      "1700\n",
      "number of samples in the training set: 1530\n",
      "number of samples in the validation set: 170\n",
      "number of samples in a batch 2\n",
      "number of samples in a batch 1\n",
      "number of classes 17\n",
      "L20-100-s100 \n",
<<<<<<< HEAD
      " /Users/danlo/Documents/PX319MLPhases_Data/L20-100-s100\n",
      "PyTorch-resnet50-111111-e1 \n",
      " Model_PyTorch-resnet50-111111-e1_L20-100-s100.pth \n",
      " History_PyTorch-resnet50-111111-e1_L20-100-s100.pkl\n",
      "./L20-100-s100/ ./L20-100-s100/Model_PyTorch-resnet50-111111-e1_L20-100-s100.pth ./L20-100-s100/History_PyTorch-resnet50-111111-e1_L20-100-s100.pkl\n",
      "Epoch 1/1\n",
      "----------\n",
      "Training with 24 batches\n",
      "Training result: loss= 3.458977, accuracy= 0.067974\n",
      "Validation with 3 batches\n",
      "Validation result: val_loss= 7.669873, val_accuracy= 0.088235\n",
      "\n",
      "Training complete in 0m 43s\n",
      "Best val Acc: 0.088235\n",
      "85000\n",
      "number of samples in the training set: 76500\n",
      "number of samples in the validation set: 8500\n",
      "number of samples in a batch 1196\n",
      "number of samples in a batch 133\n",
      "number of classes 18\n",
      "L20-5000-s100 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L20-5000-s100\n",
      "PyTorch-resnet50-111111-e1 \n",
      " Model_PyTorch-resnet50-111111-e1_L20-5000-s100.pth \n",
      " History_PyTorch-resnet50-111111-e1_L20-5000-s100.pkl\n",
      "./L20-5000-s100/ ./L20-5000-s100/Model_PyTorch-resnet50-111111-e1_L20-5000-s100.pth ./L20-5000-s100/History_PyTorch-resnet50-111111-e1_L20-5000-s100.pkl\n",
      "Epoch 1/1\n",
      "----------\n",
      "Training with 1196 batches\n",
      "Training result: loss= 2.724071, accuracy= 0.110431\n",
      "Validation with 133 batches\n",
      "Validation result: val_loss= 10.970323, val_accuracy= 0.101176\n",
      "\n",
      "Training complete in 262m 17s\n",
      "Best val Acc: 0.101176\n",
      "8501\n",
      "number of samples in the training set: 7651\n",
      "number of samples in the validation set: 850\n",
      "number of samples in a batch 120\n",
      "number of samples in a batch 14\n",
      "number of classes 17\n",
      "L30-500-s100 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L30-500-s100\n",
      "PyTorch-resnet50-111111-e1 \n",
      " Model_PyTorch-resnet50-111111-e1_L30-500-s100.pth \n",
      " History_PyTorch-resnet50-111111-e1_L30-500-s100.pkl\n",
      "./L30-500-s100/ ./L30-500-s100/Model_PyTorch-resnet50-111111-e1_L30-500-s100.pth ./L30-500-s100/History_PyTorch-resnet50-111111-e1_L30-500-s100.pkl\n",
      "Epoch 1/1\n",
      "----------\n",
      "Training with 120 batches\n",
      "Training result: loss= 2.883853, accuracy= 0.108613\n",
      "Validation with 14 batches\n",
      "Validation result: val_loss= 3.043128, val_accuracy= 0.118824\n",
      "\n",
      "Training complete in 5m 33s\n",
      "Best val Acc: 0.118824\n",
      "8501\n",
      "number of samples in the training set: 7651\n",
      "number of samples in the validation set: 850\n",
      "number of samples in a batch 120\n",
      "number of samples in a batch 14\n",
      "number of classes 17\n",
      "L30-500-s200 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L30-500-s200\n",
      "PyTorch-resnet50-111111-e1 \n",
      " Model_PyTorch-resnet50-111111-e1_L30-500-s200.pth \n",
      " History_PyTorch-resnet50-111111-e1_L30-500-s200.pkl\n",
      "./L30-500-s200/ ./L30-500-s200/Model_PyTorch-resnet50-111111-e1_L30-500-s200.pth ./L30-500-s200/History_PyTorch-resnet50-111111-e1_L30-500-s200.pkl\n",
      "Epoch 1/1\n",
      "----------\n",
      "Training with 120 batches\n",
      "Training result: loss= 3.052874, accuracy= 0.098549\n",
      "Validation with 14 batches\n",
      "Validation result: val_loss= 5.704384, val_accuracy= 0.090588\n",
      "\n",
      "Training complete in 6m 55s\n",
      "Best val Acc: 0.090588\n",
      "8501\n",
      "number of samples in the training set: 7651\n",
      "number of samples in the validation set: 850\n",
      "number of samples in a batch 120\n",
      "number of samples in a batch 14\n",
      "number of classes 17\n",
      "L30-500-s500 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L30-500-s500\n",
      "PyTorch-resnet50-111111-e1 \n",
      " Model_PyTorch-resnet50-111111-e1_L30-500-s500.pth \n",
      " History_PyTorch-resnet50-111111-e1_L30-500-s500.pkl\n",
      "./L30-500-s500/ ./L30-500-s500/Model_PyTorch-resnet50-111111-e1_L30-500-s500.pth ./L30-500-s500/History_PyTorch-resnet50-111111-e1_L30-500-s500.pkl\n",
      "Epoch 1/1\n",
      "----------\n",
      "Training with 120 batches\n",
      "Training result: loss= 2.968602, accuracy= 0.109528\n",
      "Validation with 14 batches\n",
      "Validation result: val_loss= 3.930214, val_accuracy= 0.120000\n",
      "\n",
      "Training complete in 10m 2s\n",
      "Best val Acc: 0.120000\n",
      "85000\n",
      "number of samples in the training set: 76500\n",
      "number of samples in the validation set: 8500\n",
      "number of samples in a batch 1196\n",
      "number of samples in a batch 133\n",
      "number of classes 18\n",
      "L30-5000-s100 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L30-5000-s100\n",
      "PyTorch-resnet50-111111-e1 \n",
      " Model_PyTorch-resnet50-111111-e1_L30-5000-s100.pth \n",
      " History_PyTorch-resnet50-111111-e1_L30-5000-s100.pkl\n",
      "./L30-5000-s100/ ./L30-5000-s100/Model_PyTorch-resnet50-111111-e1_L30-5000-s100.pth ./L30-5000-s100/History_PyTorch-resnet50-111111-e1_L30-5000-s100.pkl\n",
      "Epoch 1/1\n",
      "----------\n",
      "Training with 1196 batches\n",
      "51 %\r"
=======
      " /storage/disqs/ML-Data/Anderson/Images/L20-100-s100\n",
      "PyTorch-resnet50-111111-e10 \n",
      " Model_PyTorch-resnet50-111111-e10_L20-100-s100.pth \n",
      " History_PyTorch-resnet50-111111-e10_L20-100-s100.pkl\n",
      "./L20-100-s100/ ./L20-100-s100/Model_PyTorch-resnet50-111111-e10_L20-100-s100.pth ./L20-100-s100/History_PyTorch-resnet50-111111-e10_L20-100-s100.pkl\n",
      "Epoch 1/10\n",
      "----------\n",
      "Training with 2 batches\n",
      "Training result: loss= 3.165164, accuracy= 0.063399\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 53.336258, val_accuracy= 0.064706\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Training with 2 batches\n",
      "Training result: loss= 3.208571, accuracy= 0.058170\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2109.272217, val_accuracy= 0.058824\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Training with 2 batches\n",
      "Training result: loss= 3.012859, accuracy= 0.074510\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 16.705811, val_accuracy= 0.064706\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Training with 2 batches\n",
      "Training result: loss= 2.849545, accuracy= 0.099346\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 3.141352, val_accuracy= 0.070588\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Training with 2 batches\n",
      "Training result: loss= 2.815949, accuracy= 0.130065\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 43.689030, val_accuracy= 0.100000\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Training with 2 batches\n",
      "Training result: loss= 2.977618, accuracy= 0.116340\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 3.121444, val_accuracy= 0.064706\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Training with 2 batches\n",
      "Training result: loss= 2.731935, accuracy= 0.149673\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.959228, val_accuracy= 0.058824\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Training with 2 batches\n",
      "Training result: loss= 2.785492, accuracy= 0.150327\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.764493, val_accuracy= 0.064706\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Training with 2 batches\n",
      "Training result: loss= 2.627159, accuracy= 0.188235\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.741486, val_accuracy= 0.064706\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Training with 2 batches\n",
      "Training result: loss= 2.552513, accuracy= 0.204575\n",
      "Validation with 1 batches\n",
      "Validation result: val_loss= 2.709937, val_accuracy= 0.076471\n",
      "\n",
      "Training complete in 0m 19s\n",
<<<<<<< HEAD
      "Best val Acc: 0.100000\n",
      "[tensor(0.0647, device='cuda:0', dtype=torch.float64), tensor(0.0588, device='cuda:0', dtype=torch.float64), tensor(0.0647, device='cuda:0', dtype=torch.float64), tensor(0.0706, device='cuda:0', dtype=torch.float64), tensor(0.1000, device='cuda:0', dtype=torch.float64), tensor(0.0647, device='cuda:0', dtype=torch.float64), tensor(0.0588, device='cuda:0', dtype=torch.float64), tensor(0.0647, device='cuda:0', dtype=torch.float64), tensor(0.0647, device='cuda:0', dtype=torch.float64), tensor(0.0765, device='cuda:0', dtype=torch.float64)]\n",
      "0.07647058823529412\n",
      "val_accuracy: [0.17823529411764705, 0.24423529411764708, 0.1941176470588235, 0.13294117647058823, 0.19588235294117645, 0.2346711259754738, 0.11647058823529412, 0.14952941176470588, 0.18011764705882352, 0.1364705882352941, 0.22617647058823528, 0.13999999999999999, 0.07647058823529412]\n",
      "val_loss: [2.217613557927749, 2.0728614924935735, 2.1626212737139534, 2.4789557456970215, 2.2408316079308004, 2.3164597743065194, 2.799273626103121, 2.4445401241078097, 2.277799804126515, 2.5049517154693604, 2.0979786485784193, 2.4771270751953125, 2.709937334060669]\n"
=======
      "Best val Acc: 0.111765\n",
      "[tensor(0.0647, device='cuda:0', dtype=torch.float64), tensor(0.0471, device='cuda:0', dtype=torch.float64), tensor(0.0647, device='cuda:0', dtype=torch.float64), tensor(0.0588, device='cuda:0', dtype=torch.float64), tensor(0.0706, device='cuda:0', dtype=torch.float64), tensor(0.0882, device='cuda:0', dtype=torch.float64), tensor(0.0706, device='cuda:0', dtype=torch.float64), tensor(0.0765, device='cuda:0', dtype=torch.float64), tensor(0.1118, device='cuda:0', dtype=torch.float64), tensor(0.1000, device='cuda:0', dtype=torch.float64)]\n",
      "0.1\n",
      "val_accuracy: [0.16882352941176468, 0.2378823529411765, 0.19058823529411764, 0.14352941176470588, 0.18352941176470586, 0.22895763656633222, 0.12058823529411765, 0.156, 0.1783529411764706, 0.12823529411764706, 0.23117647058823526, 0.1376470588235294, 0.1]\n",
      "val_loss: [2.2368636742760155, 2.0642253231721766, 2.2532128524780273, 2.444448232650757, 2.262477726655848, 2.1249581142146985, 2.6285354434742647, 2.4185209758982937, 2.3103490770003376, 2.502702474594116, 2.1016022525114173, 2.534409284591675, 2.7268640995025635]\n"
>>>>>>> f9c06db4861f5381ec2dc58e16e68fcc556f7cf5
>>>>>>> fe9b14eb430006c3f8052c796b3573ba3afa449b
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# num_epochs=10\n",
    "val_accuracy_list = []\n",
    "val_loss_list     = []\n",
    "\n",
    "myepochs = num_epochs\n",
    "\n",
    "mysavepath = '/MachineLearning-Anderson/src/plots/performance_metrics/System_density_Metric/'\n",
    "\n",
    "for image_index in image_categories: \n",
    "    img_sizeY= img_sizeX\n",
    "\n",
    "    dataname=image_index\n",
    "    \n",
    "    #datasrc  = '/storage/disqs/'+'ML-Data/Anderson/Images/'\n",
    "    #datapath = datasrc+dataname # SC-RTP\n",
    "    datapath = datasrc+dataname # Local Machine (Laptop)\n",
    "    devhomepath = '/home/epp/phupqr/PX319MLPhases/'\n",
    "    \n",
    "    set_new_training_data(datapath)\n",
    "\n",
    "    print(dataname,\"\\n\",datapath)\n",
    "\n",
    "    modelname = 'Model_'+method+'_'+dataname+'.pth'\n",
    "    historyname = 'History_'+method+'_'+dataname+'.pkl'\n",
    "    print(method,\"\\n\",modelname,\"\\n\",historyname)\n",
    "\n",
    "    modelpath = mysavepath+modelname\n",
    "    historypath = mysavepath+historyname\n",
    "    print(mysavepath,modelpath,historypath)\n",
    "    base_model = train_model(\n",
    "        model, criterion, optimizer,num_epochs,exp_lr_scheduler,batch_size=batch_size )\n",
    "    val_accuracy_list.append(val_accuracy[-1].item())\n",
    "    print(val_accuracy)\n",
    "    print(val_accuracy[-1].item())\n",
    "    val_loss_list.append(val_loss[-1])\n",
    "print(\"val_accuracy:\", val_accuracy_list)\n",
    "print(\"val_loss:\", val_loss_list)\n",
    "fig=plt.figure()\n",
    "plt.plot(pixel_density, val_accuracy_list, '.r')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('system_density')\n",
    "plt.title('System \\'Density\\' Against Validation Accuracy')\n",
    "plt.show()\n",
    "fig.savefig(devhomepath + mysavepath + 'sys_density_val_accuracy-' + 'e' + str(myepochs) + '-' + 'seed_' + str(myseed) + '.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1\n",
      "0   10000.0  0.178235\n",
      "1   10000.0  0.244235\n",
      "2   15000.0  0.194118\n",
      "3   15000.0  0.132941\n",
      "4    5000.0  0.195882\n",
      "5    5000.0  0.234671\n",
      "6    2000.0  0.116471\n",
      "7    3000.0  0.149529\n",
      "8    4000.0  0.180118\n",
      "9    3000.0  0.136471\n",
      "10   5000.0  0.226176\n",
      "11   6000.0  0.140000\n",
      "12   2000.0  0.076471\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#a = [1,2,3]\n",
    "#b = [5,5,6]\n",
    "#nsam_res = \"-5000-s100\"\n",
    "data_csv = pd.DataFrame( np.array(pixel_density) )\n",
    "data_csv[1] = np.array(val_accuracy_list)\n",
    "print(data_csv)\n",
    "data_csv.to_csv(devhomepath + mysavepath + 'sys_metric_val_accuracy-' + 'e' + str(myepochs) + '-' + '-' + 'seed_' + str(myseed) + '.csv')\n",
    "\n",
    "#data_csv = pd.DataFrame( np.array(sys_sizes) )\n",
    "#data_csv[1] = np.array(sys_loss)\n",
    "#print(data_csv)\n",
    "#data_csv.to_csv(devhomepath + 'MachineLearning-Anderson/src/plots/performance_metrics/' + 'sys_size_loss-' + 'e' + str(myepochs) + '-' + nsam_res + '-' + 'seed_' + str(myseed) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/MachineLearning-Anderson/src/plots/performance_metrics/System_density_Metric/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fig=plt.figure()\n",
    "plt.plot(pixel_density, val_accuracy_list, '.r')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('system_density')\n",
    "plt.title('System \\'Density\\' Against Validation Accuracy')\n",
    "plt.show()\n",
    "fig.savefig(devhomepath + mysavepath + 'sys_density_val_accuracy-' + 'e' + str(myepochs) + '-' + 'seed_' + str(myseed) + '.png')\n",
    "print(mysavepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "plt.plot(epochs,val_loss, label='val loss')\n",
    "plt.plot(epochs,_loss, label='training loss')\n",
    "plt.legend(loc='upper left')\n",
    "fig.savefig(mysavepath+method+'_'+method+'_'+dataname+'_loss'+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "plt.plot(val_epochs,val_accuracy, label='val accuracy')\n",
    "#print(val_accuracy[0].item())\n",
    "plt.plot(epochs,accuracy, label='training accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "fig.savefig(savepath+method+'_'+method+'_'+dataname+'_accuracy'+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = './Anderson_'+dataname+'_resnet50_'+str(num_epochs)+'_epochs_batch_size'+str(batch_size)+'.pth'\n",
    "torch.save(model.state_dict(), modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing the quality of the learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(number_classes))\n",
    "class_total = list(0. for i in range(number_classes))\n",
    "accuracy=list(0. for i in range(number_classes))\n",
    "average=list(0. for i in range(number_classes))\n",
    "with torch.no_grad():\n",
    "     for i, (data) in enumerate(val):\n",
    "        inputs=data[0]\n",
    "        labels=data[1]\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1) \n",
    "\n",
    "        c = (preds == labels).squeeze()\n",
    "        for i in range(inputs.size()[0]):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(number_classes):\n",
    "    average[i]=(class_correct[i] / class_total[i])*100\n",
    "    \n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        class_names[i], 100 * class_correct[i] / class_total[i]))\n",
    "\n",
    "print(len(average))\n",
    "#fig=plt.figure()\n",
    "plt.figure(figsize=(14,14))\n",
    "plt.plot(class_names,average)\n",
    "plt.savefig(savepath+method+'_'+dataname+'_classacc'+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def simple_confusion_matrix(model,loader):\n",
    "    confusion_matrix = torch.zeros(number_classes, number_classes)\n",
    "    for i, (data) in enumerate(loader):\n",
    "        inputs=data[0]\n",
    "        labels=data[1]\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1) \n",
    "        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "            confusion_matrix[t.long(), p.long()] += 1\n",
    "                \n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm=simple_confusion_matrix(model,val)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_torch(cm, target_names,cmap=None,title='Confusion Matrix'):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    #accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    #misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    fig=plt.figure(figsize=(25, 25))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)#,fontsize=40)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45, fontsize=30)\n",
    "        plt.yticks(tick_marks, target_names,fontsize=30)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j]),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if  cm[i, j] == 0 or cm[i, j] > thresh else \"black\") \n",
    "\n",
    "#     fig=plt.figure()\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize=40)\n",
    "    plt.xlabel('Predicted label',fontsize=40)\n",
    "#     plt.show()\n",
    "    plt.savefig(savepath+method+'_'+dataname+'_CM'+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_torch(cm,class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.notebook.save_notebook()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
