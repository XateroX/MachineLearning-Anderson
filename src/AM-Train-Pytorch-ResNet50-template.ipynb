{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch code percolation model with ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameter choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L20-100-s100 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L20-100-s100\n",
      "PyTorch-resnet50-111111-e1 \n",
      " Model_PyTorch-resnet50-111111-e1_L20-100-s100.pth \n",
      " History_PyTorch-resnet50-111111-e1_L20-100-s100.pkl\n",
      "./L20-100-s100/ ./L20-100-s100/Model_PyTorch-resnet50-111111-e1_L20-100-s100.pth ./L20-100-s100/History_PyTorch-resnet50-111111-e1_L20-100-s100.pkl\n"
     ]
    }
   ],
   "source": [
    "myseed=111111\n",
    "width= 20\n",
    "nimages= 100\n",
    "img_sizeX= 100\n",
    "img_sizeY= img_sizeX\n",
    "\n",
    "validation_split= 0.1\n",
    "batch_size= 32*2\n",
    "num_epochs= 1\n",
    "# mylr= 0.01\n",
    "# mywd= 1e-6\n",
    "\n",
    "dataname='L'+str(width)+'-'+str(nimages)+'-s'+str(img_sizeX)\n",
    "\n",
    "#datasrc  = '/storage/disqs/'+'ML-Data/Anderson/Images/'\n",
    "#datapath = datasrc+dataname # SC-RTP\n",
    "datasrc  = '/Users/danlo/Documents/PX319MLPhases_Data/'\n",
    "datapath = datasrc+dataname # Local Machine (Laptop)\n",
    "    \n",
    "print(dataname,\"\\n\",datapath)\n",
    "\n",
    "method='PyTorch-resnet50-'+str(myseed)+'-e'+str(num_epochs) #+'-bs'+str(batch_size)\n",
    "modelname = 'Model_'+method+'_'+dataname+'.pth'\n",
    "historyname = 'History_'+method+'_'+dataname+'.pkl'\n",
    "print(method,\"\\n\",modelname,\"\\n\",historyname)\n",
    "\n",
    "savepath = './'+dataname+'/'\n",
    "import os\n",
    "try:\n",
    "    os.mkdir(savepath)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "modelpath = savepath+modelname\n",
    "historypath = savepath+historyname\n",
    "print(savepath,modelpath,historypath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed=42\n",
    "import torch\n",
    "torch.manual_seed(myseed)\n",
    "import numpy as np\n",
    "np.random.seed(myseed+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.4.0\n",
      "sklearn version: 0.23.2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "# import os\n",
    "print('torch version:',torch.__version__)\n",
    "import sklearn\n",
    "print('sklearn version:', sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device:  cpu torch.float32 torch.strided\n",
      "chosen device:  cpu\n"
     ]
    }
   ],
   "source": [
    "t=torch.Tensor()\n",
    "print('current device: ', t.device, t.dtype, t.layout)\n",
    "\n",
    "# switch to GPU if available\n",
    "device=t.device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "print('chosen device: ',device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## starting the main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L20-100-s100', 'L20-5000-s100', 'L30-500-s100', 'L30-500-s200', 'L30-500-s500', 'L30-5000-s100', 'L40-5000-s100', 'L50-1000-s100', 'L50-1000-s200', 'L50-1000-s300', 'L50-2000-s100', 'L50-5000-s100', 'L50-5000-s200']\n",
      "[5.0, 5.0, 3.3333333333333335, 6.666666666666667, 16.666666666666668, 3.3333333333333335, 2.5, 2.0, 4.0, 6.0, 2.0, 2.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "#Finding the image categories to train on\n",
    "\n",
    "#datasrc  = '/storage/disqs/'+'ML-Data/Anderson/Images/'\n",
    "#datapath = datasrc+dataname # SC-RTP\n",
    "datasrc  = '/Users/danlo/Documents/PX319MLPhases_Data/'\n",
    "datapath = datasrc+dataname # Local Machine (Laptop)\n",
    "\n",
    "image_categories = []\n",
    "pixel_density = []\n",
    "for entry in os.scandir(datasrc):\n",
    "    image_name = entry.path.rsplit('/', 1)[1]\n",
    "\n",
    "    if image_name[0] == 'L':\n",
    "        pixel_num = float(image_name.rsplit('s', 1)[1])\n",
    "        size_num = 0\n",
    "        size_bool = re.search('L(.+?)-', image_name)\n",
    "        if size_bool:\n",
    "            size_num = int(size_bool.group(1))\n",
    "        pixel_dens = pixel_num/size_num\n",
    "        pixel_density.append(pixel_dens)\n",
    "        image_categories.append(image_name)\n",
    "print(image_categories)\n",
    "print(pixel_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_sizeX= 100\n",
    "# img_sizeY= 100 #img_sizeX\n",
    "# validation_split= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyImageFolder2(torchvision.datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(MyImageFolder2, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        landmarks = landmarks * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'landmarks': landmarks}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'landmarks': torch.from_numpy(landmarks)}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "training_set=0\n",
    "validation_set=0\n",
    "# dataname='L20-100-s100'\n",
    "# path='../../../../../../../media/phrhmb/Datasets_DisQS/'+dataname\n",
    "# path='/storage/disqs/'+'ML-Data/Anderson/Images/'+dataname\n",
    "\n",
    "transform=transforms.Compose([torchvision.transforms.Resize((32,32)),\n",
    "                              torchvision.transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\danlo\\\\Documents\\\\PX319MLPhases\\\\MachineLearning-Anderson\\\\src'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#path = datapath \n",
    "whole_dataset=MyImageFolder2(root=datapath, transform=transform)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_size = len(whole_dataset)\n",
    "print(data_size)\n",
    "# validation_split=0.1\n",
    "split=int(np.floor(validation_split*data_size))\n",
    "training=int(data_size-split)\n",
    "# split the data into training and validation\n",
    "training_set, validation_set= torch.utils.data.random_split(whole_dataset,(training,split))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# batch_size=1024\n",
    "train = torch.utils.data.DataLoader(\n",
    "        dataset=training_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "# enum_train=enumerate(train)\n",
    "# len_train=len(train)\n",
    "\n",
    "val = torch.utils.data.DataLoader(\n",
    "        dataset=validation_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False)\n",
    "# enum_val=enumerate(val)\n",
    "# len_val=len(val)\n",
    "# print(len_train,len_val)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class_names =whole_dataset.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inputs,labels,path= next(iter(val))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "img_sizeX,img_sizeY= inputs.shape[-1],inputs.shape[-2]\n",
    "img_sizeY"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_of_train_samples = len(training_set) # total training samples\n",
    "num_of_test_samples = len(validation_set) #total validation samples\n",
    "steps_per_epoch = np.ceil(num_of_train_samples // batch_size)\n",
    "len_train=len(train)\n",
    "len_val=len(val)\n",
    "number_classes = len(class_names)\n",
    "print('number of samples in the training set:', num_of_train_samples)\n",
    "print('number of samples in the validation set:', num_of_test_samples )\n",
    "print('number of samples in a batch',len_train) \n",
    "print('number of samples in a batch',len_val)\n",
    "print('number of classes',number_classes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_new_training_data(datapath):\n",
    "\n",
    "    global training_set\n",
    "    training_set=0\n",
    "    global validation_set\n",
    "    validation_set=0\n",
    "    # dataname='L20-100-s100'\n",
    "    # path='../../../../../../../media/phrhmb/Datasets_DisQS/'+dataname\n",
    "    # path='/storage/disqs/'+'ML-Data/Anderson/Images/'+dataname\n",
    "\n",
    "    global transform\n",
    "    transform=transforms.Compose([torchvision.transforms.Resize((32,32)),\n",
    "                                  torchvision.transforms.ToTensor()])\n",
    "\n",
    "\n",
    "    #path = datapath \n",
    "    global whole_dataset\n",
    "    whole_dataset=MyImageFolder2(root=datapath, transform=transform)\n",
    "\n",
    "\n",
    "    global data_size\n",
    "    data_size = len(whole_dataset)\n",
    "    print(data_size)\n",
    "    # validation_split=0.1\n",
    "    global split\n",
    "    split=int(np.floor(validation_split*data_size))\n",
    "    global training\n",
    "    training=int(data_size-split)\n",
    "    # split the data into training and validation\n",
    "    training_set, validation_set= torch.utils.data.random_split(whole_dataset,(training,split))\n",
    "\n",
    "\n",
    "    # batch_size=1024\n",
    "    global train\n",
    "    train = torch.utils.data.DataLoader(\n",
    "            dataset=training_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True)\n",
    "    # enum_train=enumerate(train)\n",
    "    # len_train=len(train)\n",
    "\n",
    "    global val\n",
    "    val = torch.utils.data.DataLoader(\n",
    "            dataset=validation_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False)\n",
    "    # enum_val=enumerate(val)\n",
    "    # len_val=len(val)\n",
    "    # print(len_train,len_val)\n",
    "\n",
    "\n",
    "    global class_names\n",
    "    class_names =whole_dataset.classes\n",
    "    class_names\n",
    "\n",
    "\n",
    "    global inputs,labels,path\n",
    "    inputs,labels,path= next(iter(val))\n",
    "    labels.shape\n",
    "\n",
    "\n",
    "    global img_sizeX,img_sizeY\n",
    "    img_sizeX,img_sizeY= inputs.shape[-1],inputs.shape[-2]\n",
    "    img_sizeY\n",
    "\n",
    "\n",
    "    global num_of_train_samples\n",
    "    num_of_train_samples = len(training_set) # total training samples\n",
    "    global num_of_test_samples\n",
    "    num_of_test_samples = len(validation_set) #total validation samples\n",
    "    global steps_per_epoch\n",
    "    steps_per_epoch = np.ceil(num_of_train_samples // batch_size)\n",
    "    global len_train\n",
    "    len_train=len(train)\n",
    "    global len_val\n",
    "    len_val=len(val)\n",
    "    global number_classes\n",
    "    number_classes= len(class_names)\n",
    "    print('number of samples in the training set:', num_of_train_samples)\n",
    "    print('number of samples in the validation set:', num_of_test_samples )\n",
    "    print('number of samples in a batch',len_train) \n",
    "    print('number of samples in a batch',len_val)\n",
    "    print('number of classes',number_classes )\n",
    "    \n",
    "    global model\n",
    "    model=models.resnet50(pretrained=True, progress=True)\n",
    "\n",
    "    global num_ftrs\n",
    "    num_ftrs = model.fc.in_features # number of input features of the last layer which is fully connected (fc)\n",
    "\n",
    "    #We modify the last layer in order to have 2 output: percolating or not\n",
    "    model.fc=nn.Linear(num_ftrs, number_classes )\n",
    "     #the model is sent to the GPU\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # defining the optimizer\n",
    "\n",
    "    global optimizer\n",
    "    optimizer=torch.optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "    # defining the loss function\n",
    "    global criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    global exp_lr_scheduler\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    # checking if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    #print(model)\n",
    "    \n",
    "    #the model is sent to the GPU\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building the CNN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model=models.resnet50(pretrained=True, progress=True)\n",
    "\n",
    "num_ftrs = model.fc.in_features # number of input features of the last layer which is fully connected (fc)\n",
    "\n",
    "#We modify the last layer in order to have 2 output: percolating or not\n",
    "model.fc=nn.Linear(num_ftrs, number_classes )\n",
    " #the model is sent to the GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# defining the optimizer\n",
    "\n",
    "optimizer=torch.optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "# defining the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print model's state_dict\n",
    "# print(\"Model's state_dict:\")\n",
    "# for param_tensor in model.state_dict():\n",
    "#     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# # Print optimizer's state_dict\n",
    "# print(\"Optimizer's state_dict:\")\n",
    "# for var_name in optimizer.state_dict():\n",
    "#     print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.parameters():\n",
    "#     print(param.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#the model is sent to the GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,criterion, optimizer, num_epochs, scheduler, batch_size):\n",
    "    global accuracy, _loss, val_accuracy, val_loss, epochs, val_epochs \n",
    "    since=time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    accuracy=[]\n",
    "    _loss=[]\n",
    "    val_accuracy=[]\n",
    "    val_loss=[]\n",
    "    epochs=[]\n",
    "    val_epochs=[]\n",
    "       \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1,num_epochs))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        #two phases: training and validating\n",
    "        for phase in [train,val]:\n",
    "            if phase == train:\n",
    "                print('Training', end=\" \")\n",
    "                model.train() # set the model to training mode\n",
    "                batches= len_train\n",
    "            else:\n",
    "                print('Validation', end=\" \")\n",
    "                model.eval() # set the model to evaluation mode\n",
    "                batches= len_val\n",
    "                \n",
    "            #batches= len(list(enum_data))\n",
    "            print('with', batches, 'batches')\n",
    "            running_loss=0.0\n",
    "            running_corrects=0.0\n",
    "            \n",
    "            # Here's where the training happens\n",
    "            # print('--- iterating through data ...')\n",
    "            \n",
    "            for i, (inputs,labels,paths) in enumerate(phase):\n",
    "                \n",
    "                print(i*100//batches, '%', end=\"\\r\", flush=True)\n",
    "                \n",
    "                inputs=inputs.to(device)\n",
    "                labels=labels.to(device)\n",
    "                #paths=paths.to(device)\n",
    "                                \n",
    "                #put the gradient to zero to avoid accumulation during back propagation\n",
    "                optimizer.zero_grad()\n",
    "                                \n",
    "                #now we need to carry out the forward and backward process in different steps\n",
    "                #First the forward training\n",
    "                #for the training step we need to log the loss\n",
    "                with torch.set_grad_enabled(phase==train):\n",
    "                    outputs=model(inputs)\n",
    "                    _, preds= torch.max(outputs,1)\n",
    "                    loss=criterion(outputs,labels)\n",
    "                \n",
    "                #still for the training phase we need to implement backword process and optimization\n",
    "                \n",
    "                    if phase==train:\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                # We want variables to hold the loss statistics\n",
    "                #loss.item() extract the loss value as float then it is multiply by the batch size\n",
    "                running_loss+=loss.item()*inputs.size(0)\n",
    "                running_corrects+= torch.sum(preds==labels.data)\n",
    "                \n",
    "            if phase == train:\n",
    "                scheduler.step()            \n",
    "            \n",
    "            if phase == train:\n",
    "                epoch_loss= running_loss/len(phase.dataset)\n",
    "                epoch_acc = running_corrects.double()/ len(phase.dataset)\n",
    "                print('{} loss= {:4f}, accuracy= {:4f}'.format(\n",
    "                    'Training result:', epoch_loss, epoch_acc))\n",
    "                accuracy.append(epoch_acc)\n",
    "                _loss.append(epoch_loss)\n",
    "                epochs.append(epoch)\n",
    "                \n",
    "            if phase == val:\n",
    "                epoch_loss= running_loss/len(val.dataset)\n",
    "                epoch_acc = running_corrects.double()/len(val.dataset)\n",
    "                print('{} val_loss= {:4f}, val_accuracy= {:4f}'.format(\n",
    "                    'Validation result:', epoch_loss, epoch_acc))\n",
    "                val_accuracy.append(epoch_acc)\n",
    "                val_loss.append(epoch_loss)\n",
    "                val_epochs.append(epoch)\n",
    "                \n",
    "            # Make a copy of the model if the accuracy on the validation set has improved\n",
    "            if phase == val and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # Now we'll load in the best model weights and return it\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_handeled = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs,labels,paths) in enumerate(val):\n",
    "\n",
    "            inputs=inputs.to(device)\n",
    "            labels=labels.to(device)\n",
    "\n",
    "            outputs = model(inputs) #value of the output neurons\n",
    "            _, preds = torch.max(outputs, 1) #gives the max value and stores in preds the neurons to which it belongs\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_handeled += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_handeled)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}; \\n true label {}; \\n path: {};'.format(class_names[preds[j]] ,\n",
    "                                                                     class_names[labels[j]],paths[j])\n",
    "                            )\n",
    "                imshow(inputs.cpu().data[j])\n",
    "                \n",
    "                if images_handeled == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_misclassified(model, num_images=6): #gives shows only the misclassified images\n",
    "    import re\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs,labels,paths) in enumerate(val):\n",
    "            \n",
    "            inputs=inputs.to(device)\n",
    "            labels=labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs) #value of the output neurons\n",
    "            _, preds = torch.max(outputs, 1) #gives the max value and stores in preds the neurons to which it belongs\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                if labels[j]!=preds[j] and abs(labels[j]-preds[j])>4:\n",
    "                #print(inputs.size()[0])\n",
    "                    images_so_far += 1\n",
    "                    ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                    ax.axis('off')\n",
    "                    ax.set_title('predicted: {}; \\n true label {}; \\n path: {};'.format(class_names[preds[j]] ,\n",
    "                                                                     class_names[labels[j]],paths[j])\n",
    "                            )\n",
    "                    imshow(inputs.cpu().data[j])\n",
    "                \n",
    "                    if images_so_far == num_images:\n",
    "                        model.train(mode=was_training)\n",
    "                        return\n",
    "        \n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700\n",
      "number of samples in the training set: 1530\n",
      "number of samples in the validation set: 170\n",
      "number of samples in a batch 24\n",
      "number of samples in a batch 3\n",
      "number of classes 17\n",
      "L20-100-s100 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L20-100-s100\n",
      "PyTorch-resnet50-111111-e1 \n",
      " Model_PyTorch-resnet50-111111-e1_L20-100-s100.pth \n",
      " History_PyTorch-resnet50-111111-e1_L20-100-s100.pkl\n",
      "./L20-100-s100/ ./L20-100-s100/Model_PyTorch-resnet50-111111-e1_L20-100-s100.pth ./L20-100-s100/History_PyTorch-resnet50-111111-e1_L20-100-s100.pkl\n",
      "Epoch 1/1\n",
      "----------\n",
      "Training with 24 batches\n",
      "Training result: loss= 3.458977, accuracy= 0.067974\n",
      "Validation with 3 batches\n",
      "Validation result: val_loss= 7.669873, val_accuracy= 0.088235\n",
      "\n",
      "Training complete in 0m 43s\n",
      "Best val Acc: 0.088235\n",
      "85000\n",
      "number of samples in the training set: 76500\n",
      "number of samples in the validation set: 8500\n",
      "number of samples in a batch 1196\n",
      "number of samples in a batch 133\n",
      "number of classes 18\n",
      "L20-5000-s100 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L20-5000-s100\n",
      "PyTorch-resnet50-111111-e1 \n",
      " Model_PyTorch-resnet50-111111-e1_L20-5000-s100.pth \n",
      " History_PyTorch-resnet50-111111-e1_L20-5000-s100.pkl\n",
      "./L20-5000-s100/ ./L20-5000-s100/Model_PyTorch-resnet50-111111-e1_L20-5000-s100.pth ./L20-5000-s100/History_PyTorch-resnet50-111111-e1_L20-5000-s100.pkl\n",
      "Epoch 1/1\n",
      "----------\n",
      "Training with 1196 batches\n",
      "Training result: loss= 2.724071, accuracy= 0.110431\n",
      "Validation with 133 batches\n",
      "Validation result: val_loss= 10.970323, val_accuracy= 0.101176\n",
      "\n",
      "Training complete in 262m 17s\n",
      "Best val Acc: 0.101176\n",
      "8501\n",
      "number of samples in the training set: 7651\n",
      "number of samples in the validation set: 850\n",
      "number of samples in a batch 120\n",
      "number of samples in a batch 14\n",
      "number of classes 17\n",
      "L30-500-s100 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L30-500-s100\n",
      "PyTorch-resnet50-111111-e1 \n",
      " Model_PyTorch-resnet50-111111-e1_L30-500-s100.pth \n",
      " History_PyTorch-resnet50-111111-e1_L30-500-s100.pkl\n",
      "./L30-500-s100/ ./L30-500-s100/Model_PyTorch-resnet50-111111-e1_L30-500-s100.pth ./L30-500-s100/History_PyTorch-resnet50-111111-e1_L30-500-s100.pkl\n",
      "Epoch 1/1\n",
      "----------\n",
      "Training with 120 batches\n",
      "Training result: loss= 2.883853, accuracy= 0.108613\n",
      "Validation with 14 batches\n",
      "Validation result: val_loss= 3.043128, val_accuracy= 0.118824\n",
      "\n",
      "Training complete in 5m 33s\n",
      "Best val Acc: 0.118824\n",
      "8501\n",
      "number of samples in the training set: 7651\n",
      "number of samples in the validation set: 850\n",
      "number of samples in a batch 120\n",
      "number of samples in a batch 14\n",
      "number of classes 17\n",
      "L30-500-s200 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L30-500-s200\n",
      "PyTorch-resnet50-111111-e1 \n",
      " Model_PyTorch-resnet50-111111-e1_L30-500-s200.pth \n",
      " History_PyTorch-resnet50-111111-e1_L30-500-s200.pkl\n",
      "./L30-500-s200/ ./L30-500-s200/Model_PyTorch-resnet50-111111-e1_L30-500-s200.pth ./L30-500-s200/History_PyTorch-resnet50-111111-e1_L30-500-s200.pkl\n",
      "Epoch 1/1\n",
      "----------\n",
      "Training with 120 batches\n",
      "Training result: loss= 3.052874, accuracy= 0.098549\n",
      "Validation with 14 batches\n",
      "Validation result: val_loss= 5.704384, val_accuracy= 0.090588\n",
      "\n",
      "Training complete in 6m 55s\n",
      "Best val Acc: 0.090588\n",
      "8501\n",
      "number of samples in the training set: 7651\n",
      "number of samples in the validation set: 850\n",
      "number of samples in a batch 120\n",
      "number of samples in a batch 14\n",
      "number of classes 17\n",
      "L30-500-s500 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L30-500-s500\n",
      "PyTorch-resnet50-111111-e1 \n",
      " Model_PyTorch-resnet50-111111-e1_L30-500-s500.pth \n",
      " History_PyTorch-resnet50-111111-e1_L30-500-s500.pkl\n",
      "./L30-500-s500/ ./L30-500-s500/Model_PyTorch-resnet50-111111-e1_L30-500-s500.pth ./L30-500-s500/History_PyTorch-resnet50-111111-e1_L30-500-s500.pkl\n",
      "Epoch 1/1\n",
      "----------\n",
      "Training with 120 batches\n",
      "Training result: loss= 2.968602, accuracy= 0.109528\n",
      "Validation with 14 batches\n",
      "Validation result: val_loss= 3.930214, val_accuracy= 0.120000\n",
      "\n",
      "Training complete in 10m 2s\n",
      "Best val Acc: 0.120000\n",
      "85000\n",
      "number of samples in the training set: 76500\n",
      "number of samples in the validation set: 8500\n",
      "number of samples in a batch 1196\n",
      "number of samples in a batch 133\n",
      "number of classes 18\n",
      "L30-5000-s100 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L30-5000-s100\n",
      "PyTorch-resnet50-111111-e1 \n",
      " Model_PyTorch-resnet50-111111-e1_L30-5000-s100.pth \n",
      " History_PyTorch-resnet50-111111-e1_L30-5000-s100.pkl\n",
      "./L30-5000-s100/ ./L30-5000-s100/Model_PyTorch-resnet50-111111-e1_L30-5000-s100.pth ./L30-5000-s100/History_PyTorch-resnet50-111111-e1_L30-5000-s100.pkl\n",
      "Epoch 1/1\n",
      "----------\n",
      "Training with 1196 batches\n",
      "51 %\r"
     ]
    }
   ],
   "source": [
    "# num_epochs=10\n",
    "val_accuracy_list = []\n",
    "val_loss_list     = []\n",
    "for image_index in image_categories: \n",
    "    img_sizeY= img_sizeX\n",
    "\n",
    "    dataname=image_index\n",
    "    \n",
    "    #datasrc  = '/storage/disqs/'+'ML-Data/Anderson/Images/'\n",
    "    #datapath = datasrc+dataname # SC-RTP\n",
    "    datasrc  = '/Users/danlo/Documents/PX319MLPhases_Data/'\n",
    "    datapath = datasrc+dataname # Local Machine (Laptop)\n",
    "    \n",
    "    set_new_training_data(datapath)\n",
    "\n",
    "    print(dataname,\"\\n\",datapath)\n",
    "\n",
    "    modelname = 'Model_'+method+'_'+dataname+'.pth'\n",
    "    historyname = 'History_'+method+'_'+dataname+'.pkl'\n",
    "    print(method,\"\\n\",modelname,\"\\n\",historyname)\n",
    "\n",
    "    savepath = './'+dataname+'/'\n",
    "    import os\n",
    "    try:\n",
    "        os.mkdir(savepath)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    modelpath = savepath+modelname\n",
    "    historypath = savepath+historyname\n",
    "    print(savepath,modelpath,historypath)\n",
    "    base_model = train_model(\n",
    "        model, criterion, optimizer,num_epochs,exp_lr_scheduler,batch_size=batch_size )\n",
    "    val_accuracy_list.append(val_accuracy[-1])\n",
    "    val_loss_list.append(val_loss[-1])\n",
    "print(\"val_accuracy:\", val_accuracy_list)\n",
    "print(\"val_loss:\", val_loss_list)\n",
    "fig=plt.figure()\n",
    "plt.plot(pixel_density, val_accuracy_list)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('system_density')\n",
    "plt.show()\n",
    "fig.savefig(devhomepath + 'MachineLearning-Anderson/src/plots/performance_metrics/' + 'sys_density_val_accuracy-' + 'e' + str(myepochs) + '-' + nsam_res + '-' + 'seed_' + str(myseed) + '.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "plt.plot(epochs,val_loss, label='val loss')\n",
    "plt.plot(epochs,_loss, label='training loss')\n",
    "plt.legend(loc='upper left')\n",
    "fig.savefig(savepath+method+'_'+method+'_'+dataname+'_loss'+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "plt.plot(val_epochs,val_accuracy, label='val accuracy')\n",
    "plt.plot(epochs,accuracy, label='training accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "fig.savefig(savepath+method+'_'+method+'_'+dataname+'_accuracy'+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = './Anderson_'+dataname+'_resnet50_'+str(num_epochs)+'_epochs_batch_size'+str(batch_size)+'.pth'\n",
    "torch.save(model.state_dict(), modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing the quality of the learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(number_classes))\n",
    "class_total = list(0. for i in range(number_classes))\n",
    "accuracy=list(0. for i in range(number_classes))\n",
    "average=list(0. for i in range(number_classes))\n",
    "with torch.no_grad():\n",
    "     for i, (data) in enumerate(val):\n",
    "        inputs=data[0]\n",
    "        labels=data[1]\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1) \n",
    "\n",
    "        c = (preds == labels).squeeze()\n",
    "        for i in range(inputs.size()[0]):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(number_classes):\n",
    "    average[i]=(class_correct[i] / class_total[i])*100\n",
    "    \n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        class_names[i], 100 * class_correct[i] / class_total[i]))\n",
    "\n",
    "print(len(average))\n",
    "#fig=plt.figure()\n",
    "plt.figure(figsize=(14,14))\n",
    "plt.plot(class_names,average)\n",
    "plt.savefig(savepath+method+'_'+dataname+'_classacc'+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def simple_confusion_matrix(model,loader):\n",
    "    confusion_matrix = torch.zeros(number_classes, number_classes)\n",
    "    for i, (data) in enumerate(loader):\n",
    "        inputs=data[0]\n",
    "        labels=data[1]\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1) \n",
    "        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "            confusion_matrix[t.long(), p.long()] += 1\n",
    "                \n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm=simple_confusion_matrix(model,val)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_torch(cm, target_names,cmap=None,title='Confusion Matrix'):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    #accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    #misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    fig=plt.figure(figsize=(25, 25))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)#,fontsize=40)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45, fontsize=30)\n",
    "        plt.yticks(tick_marks, target_names,fontsize=30)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j]),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if  cm[i, j] == 0 or cm[i, j] > thresh else \"black\") \n",
    "\n",
    "#     fig=plt.figure()\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize=40)\n",
    "    plt.xlabel('Predicted label',fontsize=40)\n",
    "#     plt.show()\n",
    "    plt.savefig(savepath+method+'_'+dataname+'_CM'+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_torch(cm,class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.notebook.save_notebook()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
