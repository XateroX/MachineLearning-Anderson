{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anderson model of localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameter choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L50-5000-s100 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L50-5000-s100 \n",
      " Keras-OhtsukiConv2D-111111\n",
      "/Users/danlo/Documents/PX319MLPhases_Data/L50-5000-s100 ./L50-5000-s100/Keras-OhtsukiConv2D-111111-model-L50-5000-s100.h5 ./L50-5000-s100/Keras-OhtsukiConv2D-111111-history-L50-5000-s100.pkl /Users/danlo/Documents/PX319MLPhases_Data/\n"
     ]
    }
   ],
   "source": [
    "myseed= 111111\n",
    "width= 50\n",
    "nimages= 100\n",
    "img_sizeX= 200\n",
    "img_sizeY= img_sizeX\n",
    "\n",
    "validation_split= 0.1\n",
    "batch_size= 64\n",
    "myepochs= 10\n",
    "mylr= 0.011\n",
    "mywd= 1e-6\n",
    "\n",
    "datanameformat=\"L{0}-{1}-s{2}\"\n",
    "dataname=datanameformat.format(width,nimages,img_sizeX)\n",
    "dataname=\"L50-5000-s100\"\n",
    "#datapath = '/storage/disqs/'+'ML-Data/Anderson/Images/'+dataname # SC-RTP\n",
    "#datapath = '/Users/danlo/ownCloud/' + dataname # Windows home\n",
    "#homepath = '/Users/danlo/Documents/PX319MLPhases_Data/'\n",
    "\n",
    "datapath = '/Users/danlo/Documents/PX319MLPhases_Data/' + dataname\n",
    "homepath = '/Users/danlo/Documents/PX319MLPhases_Data/'\n",
    "\n",
    "methodformat=\"Keras-OhtsukiConv2D-{0}\"\n",
    "method=methodformat.format(myseed)\n",
    "print(dataname,\"\\n\",datapath,\"\\n\",method)\n",
    "\n",
    "modelname = method+'-model-'+dataname+'.h5'\n",
    "historyname = method+'-history-'+dataname+'.pkl'\n",
    "\n",
    "savepath = './'+dataname+'/'\n",
    "import os\n",
    "try:\n",
    "    os.mkdir(savepath)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "modelpath = savepath+modelname\n",
    "historypath = savepath+historyname\n",
    "\n",
    "print(datapath,modelpath,historypath,homepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard notebook settings\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard libraries\n",
    "import numpy as np\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "import random as rn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "#np.random.seed(1337) # for reproducibility\n",
    "#np.random.seed(2000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:  2.1.0 , keras:  2.3.1\n",
      "sklearn:  0.23.2\n"
     ]
    }
   ],
   "source": [
    "#machine learning libraries\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "print(\"tensorflow: \",tf.__version__, \", keras: \", keras.__version__)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"sklearn: \", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#special subroutines\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D\n",
    "from keras.layers import Conv1D, MaxPooling2D\n",
    "from keras.layers import AveragePooling2D, Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "\n",
    "# from tensorflow.keras.layers import Dense, Conv2D\n",
    "# from tensorflow.keras.layers import Conv1D, MaxPooling2D\n",
    "# from tensorflow.keras.layers import AveragePooling2D, Flatten\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "# from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## starting the main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(myseed) # necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "rn.seed(myseed+1) # necessary for starting core Python generated random numbers in a well-defined state.\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/fchollet/keras/issues/2280#issuecomment-306959926\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.compat.v2.random.set_seed(myseed+3)\n",
    "#tf.set_random_seed(1234)\n",
    "\n",
    "#sess = tf.compat.v2.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "#K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## reading the images"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,validation_split=validation_split)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "training_set = train_datagen.flow_from_directory(datapath,\n",
    "                                                 subset='training',\n",
    "                                                 target_size = (img_sizeX,img_sizeY),\n",
    "                                                 batch_size = batch_size, \n",
    "                                                 class_mode='categorical',\n",
    "                                                shuffle=True,seed=myseed)\n",
    "\n",
    "validation_set= train_datagen.flow_from_directory(datapath, \n",
    "                                              subset='validation', \n",
    "                                              target_size = (img_sizeX,img_sizeY),\n",
    "                                              batch_size = batch_size,\n",
    "                                              class_mode='categorical',\n",
    "                                                 shuffle=False,seed=myseed)\n",
    "\n",
    "# test_set = test_datagen.flow_from_directory('data-keras-L20-100/test_set',\n",
    "#                                             target_size = (171, 171),\n",
    "#                                             batch_size = batch_size,\n",
    "#                                             class_mode='categorical',\n",
    "#                                            shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_labels = next(training_set)\n",
    "# Y_train, Y_labels = next(validation_set)\n",
    "# len(X_train),len(X_labels),len(Y_train),len(Y_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "num_of_train_samples = training_set.samples\n",
    "num_of_test_samples = validation_set.samples\n",
    "num_classes = len(validation_set.class_indices)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    for x,y in validation_set:\n",
    "        plt.imshow(x[0],cmap='hsv')\n",
    "        #plt.title('y={}'.format(y[0]))\n",
    "        plt.axis('off')\n",
    "        break\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN architecture (Ohtsuki) created successfully!\n"
     ]
    }
   ],
   "source": [
    "def create_CNN():\n",
    "    # instantiate model\n",
    "    model=Sequential()\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size=(5,5),input_shape=(img_sizeX, img_sizeY, 3),\n",
    "                     activation = 'relu',  padding='valid'))\n",
    "    model.add(Conv2D(16, kernel_size=(5,5),activation = 'relu',padding='same'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size =(2, 2), padding='same'))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "               \n",
    "    model.add(Conv2D(32,kernel_size=(3,3), activation = 'relu', padding='valid'))  \n",
    "    model.add(Conv2D(32,kernel_size=(3,3), activation = 'relu', padding='same')) \n",
    "                 \n",
    "    model.add(MaxPooling2D(pool_size =(2, 2),padding='same')) \n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(5,5), activation = 'relu',  padding='valid'))\n",
    "    model.add(Conv2D(64, kernel_size=(5,5),activation = 'relu',padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size =(2, 2),padding='same'))\n",
    "                  \n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "                             \n",
    "    model.add(Dense(units = 64, activation = 'relu'))\n",
    "    model.add(Dense(units = num_classes, activation = 'softmax')) \n",
    "    \n",
    "    return model\n",
    "\n",
    "print('CNN architecture (Ohtsuki) created successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled successfully and ready to be trained.\n"
     ]
    }
   ],
   "source": [
    "# Choose the Optimizer and the Cost Function\n",
    "\n",
    "opt = optimizers.SGD(lr=mylr, decay=mywd)\n",
    "#opt = keras.optimizers.Adam(lr=mylr, decay=mywd)\n",
    "\n",
    "def compile_model(optimizer=opt):\n",
    "    # create the mode\n",
    "    model=create_CNN()\n",
    "    # compile the model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print('Model compiled successfully and ready to be trained.')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# opt = optimizers.SGD(lr=mylr, decay=mywd)\n",
    "# model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# create the deep neural net\n",
    "model = compile_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L20-5000-s100 \n",
      " /Users/danlo/Documents/PX319MLPhases_Data/L20-5000-s100 \n",
      " Keras-OhtsukiConv2D-111111\n",
      "Found 76500 images belonging to 18 classes.\n",
      "Found 8500 images belonging to 18 classes.\n",
      "Epoch 1/10\n",
      "  21/1195 [..............................] - ETA: 24:55 - loss: 2.8843 - accuracy: 0.0610"
     ]
    }
   ],
   "source": [
    "# train DNN and store training info in history\n",
    "#sys_size_list = [20,30,40,50];\n",
    "#for (sys_size in sys_size_list):\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "sys_scores = []\n",
    "sys_loss   = []\n",
    "sys_sizes = ['20','30','40','50']\n",
    "\n",
    "for sys_size in sys_sizes:\n",
    "    dataname=\"L\"+sys_size+\"-5000-s100\"\n",
    "    datapath = '/Users/danlo/Documents/PX319MLPhases_Data/' + dataname\n",
    "    homepath = '/Users/danlo/Documents/PX319MLPhases_Data/'\n",
    "\n",
    "    methodformat=\"Keras-OhtsukiConv2D-{0}\"\n",
    "    method=methodformat.format(myseed)\n",
    "    print(dataname,\"\\n\",datapath,\"\\n\",method)\n",
    "\n",
    "    modelname = method+'-model-'+dataname+'.h5'\n",
    "    historyname = method+'-history-'+dataname+'.pkl'\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(rescale=1./255,validation_split=validation_split)\n",
    "    test_datagen  = ImageDataGenerator(rescale=1./255)\n",
    "     \n",
    "    training_set = train_datagen.flow_from_directory(datapath,\n",
    "                                                 subset='training',\n",
    "                                                 target_size = (img_sizeX,img_sizeY),\n",
    "                                                 batch_size = batch_size, \n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True,seed=myseed)\n",
    "\n",
    "    validation_set= train_datagen.flow_from_directory(datapath, \n",
    "                                                  subset='validation', \n",
    "                                                  target_size = (img_sizeX,img_sizeY),\n",
    "                                                  batch_size = batch_size,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=False,seed=myseed)\n",
    "\n",
    "    num_of_train_samples = training_set.samples\n",
    "    num_of_test_samples = validation_set.samples\n",
    "    num_classes = len(validation_set.class_indices)\n",
    "    \n",
    "    model = compile_model()\n",
    "    #print(model.summary())\n",
    "    \n",
    "        #Train the CNN\n",
    "    history = model.fit_generator(training_set,\n",
    "                         steps_per_epoch = training_set.samples // batch_size,\n",
    "                         epochs = myepochs,\n",
    "                         validation_data = validation_set,\n",
    "                         validation_steps = validation_set.samples // batch_size)\n",
    "    \n",
    "    score = model.evaluate(validation_set, verbose=1)\n",
    "    sys_scores.append(score[1])\n",
    "    sys_loss.append(score[0])\n",
    "    print(\"Accuracy: \", score[1])\n",
    "    print(\"loss: \", score[0])\n",
    "    model.save(modelpath) \n",
    "        \n",
    "print(\"Accuracy list: \", sys_scores)\n",
    "fig=plt.figure()\n",
    "plt.plot(sys_sizes, sys_scores)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('system_size')\n",
    "plt.show()\n",
    "\n",
    "fig=plt.figure()\n",
    "plt.plot(sys_size, sys_loss)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('system_size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.models.save_model(history,'Anderson_Ohtsuki_model_L20_500_keras_SGD_0_01_good_input_size.h5') \n",
    "model.save(modelpath) \n",
    "\n",
    "import pickle\n",
    "f=open(historypath,\"wb\")\n",
    "pickle.dump(history,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing the quality of the learned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping the Training, saving inbetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "for i in range(2):\n",
    "    model = load_model(modelpath)\n",
    "    history = model.fit_generator(training_set,\n",
    "                         steps_per_epoch = training_set.samples // batch_size,\n",
    "                         epochs = 10,\n",
    "                         validation_data = validation_set,\n",
    "                         validation_steps = validation_set.samples // batch_size)\n",
    "model.save(modelpath) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "score=model.evaluate(validation_set,verbose=1)\n",
    "\n",
    "# print performance\n",
    "print()\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# look into training history\n",
    "\n",
    "# summarize history for accuracy\n",
    "fig=plt.figure()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.ylabel('model accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.title(dataname)\n",
    "plt.show()\n",
    "fig.savefig(datapath+'/'+method+dataname+'_accuracy'+'.png')\n",
    "\n",
    "# summarize history for loss\n",
    "fig=plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.title(dataname)\n",
    "plt.show()\n",
    "fig.savefig(datapath+'/'+method+dataname+'_loss'+'.png')\n",
    "#print(datapath+'/'+surname+dataname+'_loss'+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set.reset()\n",
    "label=validation_set.class_indices.keys()\n",
    "\n",
    "#Confusion Matrix \n",
    "Y_pred = model.predict_generator(validation_set, num_of_test_samples // batch_size+1, verbose=1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "#basic confusion matrix\n",
    "confusion_matrix(validation_set.classes, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir('../PyCode/')\n",
    "from plot_confusion_matrix import *\n",
    "\n",
    "print(plot_confusion_matrix(confusion_matrix(validation_set.classes, y_pred),\n",
    "                          label,\n",
    "                          title='Confusion matrix for '+dataname,\n",
    "                          cmap=None,\n",
    "                          normalize=True))\n",
    "os.chdir('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
